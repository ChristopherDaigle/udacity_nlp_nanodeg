{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4f4e034-7e52-4fd3-af70-34e01a38ed86",
   "metadata": {},
   "source": [
    "# Part of Speech (POS) Tagging with Hidden Markov Models (HMMs)\n",
    "\n",
    "In this lesson, we introduce **Hidden Markov Models (HMMs)**, which are used to model sequences (including time-series data).\n",
    "\n",
    "**HMMs** have been successfully used on both supervised and unsupervised problems with sequence data in applications like speech recognition, bioinformatics, sign language recognition, and gesture recognition.\n",
    "\n",
    "This lesson is closely related to the project at the end of this course, where you will use a **Hidden Markov Model (HMM)** to tag parts of speech in English sentences. This is a common pre-processing task in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5361ce-dc13-4ead-83d2-15a0a41ea37f",
   "metadata": {},
   "source": [
    "## What are parts of speech\n",
    "\n",
    "Parts of speech are labels typically applied to words in a sentence. There are many potential parts of speech for most languages, but for this, we will focus on only three:\n",
    "\n",
    "<img src='img_40.png' width=700 align='center'>\n",
    "\n",
    "Identifything these in sentences is known as **part-of-speech tagging**:\n",
    "\n",
    "<img src='img_41.png' width=700 align='center'>\n",
    "\n",
    "where \"mary\" is a noun, \"had\" is a verb, \"a\" is a determinant, \"little\" is an adjective, and \"lamb\" is a noun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dbc517-6f89-45c7-9fb2-0f80134b33f8",
   "metadata": {},
   "source": [
    "## Using a LookUp Table to Assign POS - Rudimentary Model\n",
    "\n",
    "Say you have two sentences for training data:\n",
    "1. \"Mary saw Jane\" -> {\"Mary: \"noun\", \"saw\": \"verb\", \"Jane\": \"noun\"}\n",
    "2. \"Jane saw Will\" -> {\"Jane: \"noun\", \"saw\": \"verb\", \"Will\": \"noun\"}\n",
    "\n",
    "So, we can create a table:\n",
    "\n",
    "| | Noun | Verb|\n",
    "|:-|:-:|:-:|\n",
    "|Mary|1|0|\n",
    "|saw|0|2|\n",
    "|Jane|2|0|\n",
    "|Will|1|0|\n",
    "\n",
    "So now, if we have a sentence \"Mary saw Will\", we can use the lookup table and note:\n",
    "* Mary is a noun 100% of the time (in training data)\n",
    "* Saw is a verb 100% of the time (in training data)\n",
    "* Will is a noun 100% of the time (in training data)\n",
    "\n",
    "So we can tag this sentence as such:\n",
    "* \"Mary saw Will\" -> {\"Mary: \"noun\", \"saw\": \"verb\", \"Will\": \"noun\"}\n",
    "\n",
    "\n",
    "<img src='img_42.png' width=700 align='center'>\n",
    "\n",
    "So, that worked! However, we can't reasonably expect the LookUp Table method to work all, or maybe most, of the time\n",
    "\n",
    "Let's use a more complex sentence series now as an example:\n",
    "\n",
    "Say you have three sentences for training data:\n",
    "1. \"Mary will see Jane\" -> {\"Mary\": \"noun\", \"will\": \"modal\", \"see\": \"verb\", \"Jane\": \"noun\"}\n",
    "2. \"Will will see Mary\" -> {\"Will\": \"noun\", \"will\": \"modal\", \"see\": \"verb\", \"Mary\": \"noun\"}\n",
    "3. \"Jane will see Will\" -> {\"Jane\": \"noun\", \"will\": \"modal\", \"see\": \"verb\", \"Will\": \"noun\"}\n",
    "\n",
    "So, we can create a table:\n",
    "\n",
    "| | Noun | Verb| Modal|\n",
    "|:-|:-:|:-:|:-:|\n",
    "|Mary|3|0|0|\n",
    "|see|0|3|0|\n",
    "|Jane|2|0|0|\n",
    "|Will|2|0|3|\n",
    "\n",
    "\n",
    "Now, lets take a new sentence, \"Mary will see Will\", and tag it based on the LookUp Table:\n",
    "* Mary is a noun 100% of the time -> Mary: noun\n",
    "* will is a modal $\\frac{3}{5}$ and a noun $\\frac{2}{5}$ of the time -> will: modal\n",
    "* see is a verb 100% of the time-> see:verb\n",
    "\n",
    "So we can tag this sentence as such:\n",
    "* \"Mary will see Will\" -> {\"Mary: \"noun\", \"see\": \"verb\", \"will\": \"modal\"}\n",
    "\n",
    "But this is a problem, because we know Will is a noun and not a modal! This LookUp Table works by assigning the most prevalent POS Tag and isn't aware of context!\n",
    "\n",
    "<img src='img_43.png' width=700 align='center'>\n",
    "\n",
    "### Bi-Gram LookUp Table\n",
    "\n",
    "So, how can the LookUp table take advantage of context? We can assign frequencies instead to groups of words, namely n-grams!\n",
    "\n",
    "| | Noun-Modal (N-M)| Modal-Verb (M-V)| Verb-Noun (V-N)|\n",
    "|:-|:-:|:-:|:-:|\n",
    "|mary-will|1|0|0|\n",
    "|will-see|0|3|0|\n",
    "|see-jane|0|0|1|\n",
    "|will-will|1|0|0|\n",
    "|see-mary|0|0|1|\n",
    "|jane-will|1|0|0|\n",
    "|see-will|0|0|1|\n",
    "\n",
    "So now, lets take the same sentence as before, \"Mary will see Will\", and tag it based on the bi-gram LookUp Table:\n",
    "* mary will is a noun-modal 100% of the time -> Mary: noun, will: modal\n",
    "* will see is a modal-verb 100% of the time -> will: modal, see: verb\n",
    "* see will is a verb-noun 100% of the time -> see: verb, will: noun\n",
    "\n",
    "So: \"Mary will see Will\" -> Mary:noun, will:modal, see:verb, Will: noun\n",
    "\n",
    "<img src='img_44.png' width=700 align='center'>\n",
    "\n",
    "### But will bi-grams always work?\n",
    "\n",
    "Let's change our training data a bit:\n",
    "1. \"Mary Jane can see Will.\" -> {\"Mary\": \"noun\", \"Jane\": \"noun\", \"can\": ,\"see\": \"verb\", \"Will\": \"noun\"}\n",
    "2. \"Spot will see Mary\" -> {\"Will\": \"noun\", \"will\": \"modal\", \"see\": \"verb\", \"Mary\": \"noun\"}\n",
    "3. \"Will Jane spot Mary?\"\n",
    "4. \"Mary will pat Spot.\" -> {\"Jane\": \"noun\", \"will\": \"modal\", \"see\": \"verb\", \"Will\": \"noun\"}\n",
    "\n",
    "And let's tag the sentence: \"Jane will spot Will.\"\n",
    "\n",
    "| | Noun-Modal (N-M)| Modal-Verb (M-V)| Verb-Noun (V-N)|etc|\n",
    "|:-|:-:|:-:|:-:|:-:|\n",
    "|mary-jane|0|0|0|1|\n",
    "|jane-can|0|0|0|1|\n",
    "|can-see|0|0|0|1|\n",
    "|see-will|0|0|0|1|\n",
    "|spot-will|1|0|0|0|\n",
    "|will-see|0|1|0|0|\n",
    "|see-mary|0|0|1|0|\n",
    "|will-jane|0|0|1|0|\n",
    "|jane-spot|0|0|0|1|\n",
    "|spot-mary|0|0|1|0|\n",
    "|mary-will|1|0|0|0|\n",
    "|will-pat|0|1|0|0|\n",
    "|pat-spot|0|0|1|0|\n",
    "\n",
    "But wait, the bi-gram \"jane-will\" does not appear in the training data... so we have no way to fill in the extra information! So instead, we'll introduce **hidden Markov models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf54d99-94a3-4887-a2e8-534fea65183e",
   "metadata": {},
   "source": [
    "## Hidden Markov Models (HMM)\n",
    "\n",
    "The idea of HMM is the following: say that a way of tagging the sentence \"Jane will spot Will\" is noun-modal-verb-noun, so we'll calculate a probability associated with this tagging. So first, well ask \"how likely is it that a noun is followed by a modal and a modal is followed by a verb and a verb is followed by a noun\". These are called the **transition probabilities**. Now, the second set of probabilities we need to calculate are these: \"what is the probability that a noun will be the word *Jane* and that a modal will be the word *will*, etc.\" These also need to be relatively high for our tagging to be likely. These are called the **Emission Probabilities**\n",
    "\n",
    "<img src='img_45.png' width=700 align=\"center\">\n",
    "\n",
    "So if we have the following sentences, we can calculate the emission probabilities:\n",
    "1. Mary Jane can see Will\n",
    "2. Spot will see Mary\n",
    "3. Will Jane spot Mary\n",
    "4. Mary will pat Spot\n",
    "\n",
    "We can create a LookUp Table as such:\n",
    "\n",
    "||Noun|Modal|Verb|\n",
    "|:-|:-:|:-:|:-:|\n",
    "|Mary|4|0|0|\n",
    "|Jane|2|0|0|\n",
    "|Will|1|3|0|\n",
    "|Spot|2|0|1|\n",
    "|Can|0|1|0|\n",
    "|See|0|0|2|\n",
    "|Pat|0|0|1|\n",
    "\n",
    "To find the probabilities (frequencies), we sum the columns' values, then we divide the values in each row by that total\n",
    "* Noun=9\n",
    "* Modal=4\n",
    "* Verb=4\n",
    "\n",
    "||Noun|Modal|Verb|\n",
    "|:-|:-:|:-:|:-:|\n",
    "|Mary|$\\frac{4}{9}$|0|0|\n",
    "|Jane|$\\frac{2}{9}$|0|0|\n",
    "|Will|$\\frac{1}{9}$|$\\frac{3}{4}$|0|\n",
    "|Spot|$\\frac{2}{9}$|0|$\\frac{1}{4}$|\n",
    "|Can|0|$\\frac{1}{4}$|0|\n",
    "|See|0|0|$\\frac{2}{4}$|\n",
    "|Pat|0|0|$\\frac{1}{4}$|\n",
    "\n",
    "<img src='img_46.png' width=700 align=\"center\">\n",
    "\n",
    "Graphically:\n",
    "\n",
    "<img src='img_47.png' width=700 align=\"center\">\n",
    "\n",
    "\n",
    "Now, let's calculate the transition probabilities:\n",
    "1. Add starting `<s>` and ending `<e>` tags to each sentence and we'll treat these tags as parts of speech as well\n",
    "2. Make a table of counts!\n",
    "\n",
    "**Tag Start and End:**\n",
    "1. \\<s>Mary Jane can see Will\\<e>\n",
    "2. \\<s>Spot will see Mary\\<e>\n",
    "3. \\<s>Will Jane spot Mary\\<e>\n",
    "4. \\<s>Mary will pat Spot\\<e>\n",
    "\n",
    "**Make a table of counts:**\n",
    "\n",
    "|Preceding/Succeeding|Noun|Modal|Verb|\\<e>|\n",
    "|:-|:-:|:-:|:-:|:-:|\n",
    "|\\<s>|3|1|0|0|\n",
    "|Noun|1|3|1|4|\n",
    "|Modal|1|0|3|0|\n",
    "|Verb|4|0|0|0|\n",
    "\n",
    "Where the values going down the left are the POS tag of the preceding term and the values going across are the succeeding POS tag of the term. So, noun modal happens 3 times, just like starting tag and noun happen 3 times\n",
    "\n",
    "<img src='img_48.png' width=700 align=\"center\">\n",
    "\n",
    "Now, to find the probabilities (frequencies), the transition probabilities (matrix), we divide the sum of occurences in the **row**\n",
    "\n",
    "|Preceding/Succeeding|Noun|Modal|Verb|\\<e>|\n",
    "|:-|:-:|:-:|:-:|:-:|\n",
    "|\\<s>|$\\frac{3}{4}$|$\\frac{1}{4}$|0|0|\n",
    "|Noun|$\\frac{1}{9}$|$\\frac{3}{9}$|$\\frac{1}{9}$|$\\frac{4}{9}$|\n",
    "|Modal|$\\frac{1}{4}$|0|$\\frac{3}{4}$|0|\n",
    "|Verb|$\\frac{4}{4}$|0|0|0|\n",
    "\n",
    "So, if we take the **modal row**, the probability that a noun follows a modal is $\\frac{1}{4}$ and is $\\frac{3}{4}$ it's a verb, and is $0$ that it's a modal or the end of a sentence\n",
    "\n",
    "Now, graphically, we have:\n",
    "\n",
    "<img src='img_49.png' width=700 align=\"center\">\n",
    "\n",
    "As a final step, we combine the previous two steps (emission probabilities and transition probabilities) to get the HMM:\n",
    "1. We have our words which are the observations\n",
    "> * They are called the observations because they are the things we observe when we read the sentences\n",
    "2. The parts of speech are called the *hidden states*\n",
    "> * They are the ones we don't know and we have to infer based on the words\n",
    "3. The transition probabilities (among the hidden states)\n",
    "4. The emission probabilities (between the hidden states)\n",
    "\n",
    "<img src='img_50.png' width=700 align=\"center\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b867de2-717a-42e9-9999-757fc4536b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_nlp",
   "language": "python",
   "name": "venv_ud_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
