{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "587463f1-e98e-475a-aebf-ddc97e5dbece",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spam Classifier with Naive Bayes\n",
    "\n",
    "**Naive Bayes** is a supervised machine learning algorithm that can be trained to classify data into multi-class categories. In the heart of the Naive Bayes algorithm is the probabilistic model that computes the conditional probabilities of the input features and assigns the probability distributions to each of the possible classes.\n",
    "\n",
    "In this lesson, we will:\n",
    "* Review the conditional probability and Bayes Rule\n",
    "* Learn how the Naive Bayes algorithm works\n",
    "\n",
    "At the end of this course, you will do a coding exercise to apply Naive Bayes in a Natural Language Processing (NLP) task, ie. spam emails classification, using the Scikit-Learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb27a3-ce4d-421e-9cdb-6fa74f30be09",
   "metadata": {},
   "source": [
    "## Introduction to Bayes Theorem\n",
    "\n",
    "Suppose we are at an office and there are two people, Alex and Brenda. One of them passes us by in the hallway, but we aren't sure which it is. Without any other information, what is the probability it was Alex? Brenda?\n",
    "> *Prior*: initial 50:50 guess, it's all we can infer *prior* to new information\n",
    "\n",
    "Suppose we have some other information, like the person was wearing a red sweater. We happen to know that the frequency with which Alex and Brenda wear red sweaters each week (for whatever reason), but we don't know which days. If we know Alex wears a red sweater 2 days and Brenda wears a red sweater 3 days out of the same work week, what is the probability that Alex passed us? Brenda?\n",
    "> * *Posterior*: inferred after new information arrived\n",
    "\n",
    "<img src=\"img_18.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18715d-416d-4145-bafb-1fb46f394368",
   "metadata": {},
   "source": [
    "## How Bayes Theorem Works\n",
    "\n",
    "What Bayes Theorem does is switch from what we know to what we infer\n",
    "\n",
    "We start with knowing there are 2 people, Brenda and Alex.\n",
    "\n",
    "Then, we know the probability that Alex wears red and we know the probability that Brenda wears red.\n",
    "\n",
    "Given these probabilities, we can infer that someone wearing red is Alex, or someone wearing red is Brenda.\n",
    "\n",
    "<img src='img_19.png' width=700 align='center'>\n",
    "\n",
    "**Generalized:**\n",
    "\n",
    "Initially, we know the probability of an event, say $A$ such that $\\text{P} \\left( A \\right)$\n",
    "\n",
    "When we update that with new information, say we introduce a new event $R$ such that $\\text{P} \\left( R\\;\\middle|  A \\right)$, so we know the probability of R given A\n",
    "\n",
    "**Bayes Theorem** infers the probability of A given R, $\\text{P} \\left( A \\;\\middle| R \\right)$, which is the *new* probability of A once we know that the event R occured\n",
    "\n",
    "<img src='img_20.png' width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7497e-f54c-4b55-986a-23160fadf75f",
   "metadata": {},
   "source": [
    "## Applying Bayes Theorem\n",
    "\n",
    "If we know that Alex comes in 3 days a week and Brenda only comes to the office one day a week, then it's 3 times more likely to see Alex than it is to see Brenda on any given day of the week:\n",
    "$$\\text{Probability Alex}=\\text{P}\\left(A\\right)=0.75$$\n",
    "$$\\text{Probability Brenda}=\\text{P}\\left(B\\right)=0.25$$\n",
    "\n",
    "This is our prior\n",
    "\n",
    "Now, let's update that knowledge with the fact that the person had a red sweater. Recall, Alex wears red 2 times a week and Brenda wears red 3 times a week:\n",
    "\n",
    "$$\\text{Probability Red given it's Alex}=\\text{P}\\left(R\\mid A\\right)=\\frac{2}{5}=0.4$$\n",
    "$$\\text{Probability Red given it's Brenda}=\\text{P}\\left(R\\mid B\\right)=\\frac{3}{5}=0.6$$\n",
    "\n",
    "Taking that we are 3-times more likely to see Alex for every 1 time we see Brenda, then we can think in terms of weeks. So, take 4-weeks (3 for Alex + 1 for Brenda = 4 total, 20-days) with Alex wearing red 2-times a week (3 weeks for Alex with 2 red shirts a week, 6 times we see a red shirt in a 4 week period from Alex) and Brenda wearing red 3-times a week (1 week for Brenda wearing red 3-times a week, 3 times we see a red shirt in a 4 week period from Brenda), so 9-days (6 from Alex + 3 from Brenda) out of the 20-day period had red:\n",
    "$$\\text{Probability of Red}=\\text{P}\\left(R\\right)=\\frac{9}{20}=0.45$$\n",
    "\n",
    "Or more simply...\n",
    "$$\\text{Probability Red given it's Alex} \\cdot \\text{Probability it's Alex} + \\text{Probability Red given it's Brenda} \\cdot \\text{Probability it's Brenda} = 0.4 \\cdot 0.75 + 0.6 \\cdot 0.25 = 0.45$$\n",
    "\n",
    "\n",
    "<img src='img_21.png' width=700 alig='center'>\n",
    "\n",
    "In the 9 times someone was wearing red, 6 of them were Alex and three of them were Brenda.\n",
    "> Note, Brenda can wear red and not be at the office, but we are only interested in information about wearing red at the office.\n",
    "\n",
    "So, $\\frac{2}{3}$ of the time we saw someone wearing red, it was Alex, and $\\frac{1}{3}$ of the time it was Brenda.\n",
    "> We now update our prior to get a posterior:\n",
    "$$\\text{Probability Alex given Red}=\\text{P}\\left(A\\mid R\\right)=\\frac{2}{3}$$\n",
    "$$\\text{Probability Brenda given Red}=\\text{P}\\left(B\\mid R\\right)=\\frac{1}{3}$$\n",
    "\n",
    "We can similarly get $\\text{P}\\left(A\\mid R\\right)=\\frac{2}{3}$ and $\\text{P}\\left(B\\mid R\\right)=\\frac{1}{3}$ by:\n",
    "$$\\text{P}\\left(A\\mid R\\right)=\\frac{\\text{Probability Alex} \\cdot \\text{Probability Red given it's Alex}}{\\text{Probability Red}} = \\frac{\\text{P}\\left(A\\right) \\cdot \\text{P}\\left(R\\mid A\\right)}{\\text{P}\\left(R\\right)} = \\frac{0.75 \\cdot 0.4}{0.45}=\\frac{2}{3}$$\n",
    "$$\\text{P}\\left(B\\mid R\\right)=\\frac{\\text{Probability Brenda} \\cdot \\text{Probability Red given it's Brenda}}{\\text{Probability Red}} = \\frac{\\text{P}\\left(B\\right) \\cdot \\text{P}\\left(R\\mid B\\right)}{\\text{P}\\left(R\\right)} = \\frac{0.25 \\cdot 0.6}{0.45}=\\frac{1}{3}$$\n",
    "\n",
    "<img src='img_22.png' width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd339184-8cdd-48fb-8542-179a0711f6df",
   "metadata": {},
   "source": [
    "## Formal Representation of Bayes Theorem\n",
    "\n",
    "So we have a prior of an event A (or B), and we update that prior with some information about event R (or the complement of R, where R does not occur), and we can get the probability of R given A (or \"not R\" given A), which is the probability of the intersection of R and A.\n",
    "<img src='img_23.png' width=700 align='center'>\n",
    "\n",
    "But, we are interested for this exercise for finding out which event is more likely, A or B, given R, so we are not concerned with the complement of R:\n",
    "\n",
    "<img src='img_24.png' width=700 align='center'>\n",
    "\n",
    "By the [law of conditional probability](https://en.wikipedia.org/wiki/Conditional_probability) ($P(A|B)=\\frac{P(A \\cap B)}{P(B)} \\equiv P(R|A)=\\frac{P(R \\cap A)}{P(A)} \\rightarrow P(R|A) \\cdot P(A) = P(R \\cap A)$, we get:\n",
    "\n",
    "<img src='img_25.png' width=700 align='center'>\n",
    "\n",
    "Since these probabilities do not sum to one ($P(R|A) \\cdot P(A) + P(R|B) \\cdot P(B) \\neq 1$), we can normalize them so that the new probabilities do:\n",
    "\n",
    "<img src='img_26.png' width=700 align='center'>\n",
    "\n",
    "**Concluding:**\n",
    "* We had **prior** probabilities: $P(A)$ and $P(B)$\n",
    "* We updated them with event R to get **posterior** probabilities: $P(A|R)$ and $P(B|R)$\n",
    "\n",
    "<img src='img_27.png' width=700 align='center'>\n",
    "\n",
    "**Bayes Theorem:**\n",
    "$$\\text{P}\\left(\\text{A} \\mid \\text{B} \\right) = \\frac{\\text{P}\\left(\\text{B} \\mid \\text{A} \\right) \\cdot \\text{P}\\left(\\text{A}\\right)}{\\text{P}\\left(\\text{B}\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4895420-8289-4efb-902a-9748355072e7",
   "metadata": {},
   "source": [
    "## Another Application of the Bayes Theorem\n",
    "\n",
    "<img src='img_28.png' width=700 align=center>\n",
    "\n",
    "$$p(\\text{sick})=\\frac{1}{10,000}=0.0001=\\text{0.01%}$$\n",
    "$$p(\\text{test is positive given sick})=\\frac{99}{100}=0.99=\\text{99%}$$\n",
    "$$p(\\text{sick}\\mid\\text{test is positive})=\\frac{p(\\text{test is positive}\\mid\\text{sick})\\cdot p(\\text{sick})}{p(\\text{test is positive})}$$\n",
    "\n",
    "Let:\n",
    "$$\\text{S:Sick}$$\n",
    "$$\\text{H:Healthy}$$\n",
    "$$\\text{pos:positive result}$$\n",
    "\n",
    "So:\n",
    "$$\\text{P}\\left(\\text{S}\\right) = 0.0001 \\land \\text{P}\\left(\\text{H}\\right) = 0.9999$$\n",
    "$$\\text{P}\\left(\\text{pos}\\mid\\text{S}\\right) = 0.99 \\land \\text{P}\\left(\\text{pos}\\mid\\text{H}\\right) = 0.01$$\n",
    "$$\\text{P}\\left(\\text{pos}\\right) = \\text{P}\\left(\\text{S}\\right)\\cdot \\text{P}\\left(\\text{pos}\\mid\\text{S}\\right) + \\text{P}\\left(\\text{H}\\right)\\cdot \\text{P}\\left(\\text{pos}\\mid\\text{H}\\right) = 0.0001\\cdot0.99 + 0.9999\\cdot0.01 = 0.010098$$\n",
    "\n",
    "Then,\n",
    "$$\\text{P}\\left(\\text{S}\\mid\\text{pos}\\right) = \\frac{\\text{P}\\left(\\text{S}\\right) \\cdot \\text{P}\\left(\\text{pos}\\mid\\text{S}\\right)}{\\text{P}\\left(\\text{pos}\\right)} = \\frac{0.0001\\cdot0.99}{0.010098}=0.0098$$\n",
    "\n",
    "<img src='img_29.png' width=700 align='center'>\n",
    "\n",
    "<img src='img_30.png' width=700 align='center'>\n",
    "\n",
    "<img src='img_31.png' width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b13f1-3c9c-41ab-8599-236b075b9a5d",
   "metadata": {},
   "source": [
    "# Bayesian Learning\n",
    "\n",
    "<img src='img_32.png' width=700 align='center'>\n",
    "\n",
    "$$p(easy|spam) = \\frac{1}{3}$$\n",
    "$$p(money|spam) = \\frac{2}{3}$$\n",
    "\n",
    "<img src='img_33.png' width=700 align='center'>\n",
    "\n",
    "If we calculate out the probabilities of all, spam or ham, easy or not containing easy, we get:\n",
    "$$p(spam)=\\frac{3}{8} \\land p(ham)=\\frac{5}{8}$$\n",
    "Given spam (3 emails):\n",
    "$$p(easy)=\\frac{1}{3}$$\n",
    "\n",
    "Given ham (5 emails):\n",
    "$$p(easy)=\\frac{1}{5}$$\n",
    "\n",
    "Then:\n",
    "$$p(easy|spam)=\\frac{3}{8}\\cdot\\frac{1}{3}=\\frac{1}{8}$$\n",
    "$$p(easy|ham)=\\frac{5}{8}\\cdot\\frac{1}{5}=\\frac{1}{8}$$\n",
    "\n",
    "<img src='img_34.png' width=700 align='center'>\n",
    "\n",
    "But if we know that the email contains the word \"easy\". so our entire universe consists only of these two cases of \"easy\" being present: when the email is spam or ham. Those two have the same probabiltiy of happening, $\\frac{1}{8}$. So, once we normalize the probabilities, they both turn into 50%! Thus, our two posterior probabilities are 50%.\n",
    "\n",
    "<img src='img_35.png' width=700 align='center'>\n",
    "\n",
    "For ham emails with the word \"money\", we can do the same procedure!\n",
    "\n",
    "<img src='img_36.png' width=700 align='center'>\n",
    "\n",
    "Again, because the emails we are interested in are only those with the word \"money\", we can ignore the rest of the population! We can update our posteriors of being ham or spam given the word money to $p(spam|\"money\")=\\frac{2}{3} \\land p(ham|\"money\")=\\frac{1}{3}$\n",
    "\n",
    "<img src='img_37.png' width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cabd085-bdc4-4030-927d-ee0e7aa13f42",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "\n",
    "Where does the word \"naive\" come from in naive bayes?\n",
    "\n",
    "Lets look at the probability of two events happening together: $p(\\text{A & B})$ or $p(\\text{A}\\cap\\text{B})$ which $p(\\text{A}\\cap\\text{B})=p(A)\\cdot p(B)$ , but that is only true when the two events are independent. As an example, consider the probability of it being hot outside and the probability of it being cold outside. What's the probability of the two events happening simultaneously? 0. It cannot be both hot and cold simultaneously. So, the probability of being hot and the probabiltiy of being cold are dependent.\n",
    "\n",
    "<img src='img_38.png' width=700 align='center'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658860c2-05bf-4751-b152-3ace8153c46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_nlp",
   "language": "python",
   "name": "venv_ud_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
