{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194952ff-5c29-46d7-be7f-bbeb2af15a7e",
   "metadata": {},
   "source": [
    "# Introduction to Attention\n",
    "<!-- estimated time: 4hours -->\n",
    "\n",
    "This section will cover:\n",
    "\n",
    "1. Sequence to sequence recap\n",
    "2. Attention overview - Encoding\n",
    "3. Attention overview - Decoding\n",
    "4. Bahdanau and Luong Attention\n",
    "5. Multiplicative attention\n",
    "* Additive attention\n",
    "* Computer vision applications\n",
    "* NLP application: Google neural machine translation\n",
    "* Other attention methods\n",
    "* The transformer and self-attention\n",
    "* Lab: Attention basics\n",
    "\n",
    "Attention started out in the field of computer vision as an attempt to mimic human perception:\n",
    "> \"One important property of human percetption is that one does not tend to process a while in its entirety at once. Instead, humans focus attention selectively on parts of the visual space to acquire information when and where it is needed, and combine information from different fixations over time to build up an internal representation of the scene, guiding future eye movements and decision making\"\n",
    "- [Recurrent Models of Visual Attention](https://arxiv.org/abs/1406.6247)\n",
    "\n",
    "Note here that instead of processing the entirety of the image, all that is needed to know it is a picture of a bird is to ignore the background and instead focus on the item of interest. Further, if we can separate attention from the entirety of the image to componenets of it, we can describe the image in a more complete and nuanced manner:\n",
    "<img src=\"assets/images/06/img_001.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c63eca-d6d7-48fb-af48-e7e406a86e5c",
   "metadata": {},
   "source": [
    "# 1: Seq2Seq Recap\n",
    "\n",
    "Classic, i.e., those without attention, Seq2Seq models have to look at the original sentence that is to be translated one time and then use that *entire* input to produce every single small output term.\n",
    "\n",
    "A sequence to sequence model takes in an input that is a sequence of items and then produces another sequence of items as an output.\n",
    "\n",
    "* In machine translation, the input sequence is a series of words in one language and the output is a translation in another language.\n",
    "\n",
    "* In text summarization, the input is a long sequence of words and the output is a short one.\n",
    "\n",
    "The seq2seq model usually consists of an encoder and decoder. It works by the encoder first processing all of the inputs, turning the inputs into a single representation. Typically a single vector known as the **context** vector. The *context* vector contains whatever information the encoder was able to capture from the input sequence.\n",
    "<img src=\"assets/images/06/img_002.png\" width=700 align='center'>\n",
    "\n",
    "The context vector is then sent to the decoder which uses it to formulate an output sequence. In machine translation scenarios, the encoder and decoder are both recurrent neural networks (RNNs), usually LSTM cells (long short term memory)\n",
    "<img src=\"assets/images/06/img_003.png\" width=700 align='center'>\n",
    "\n",
    "In this scenario, the context vector is a vector of numbers encoding the information that the encoder captured from the input sequence. In real world scenarios, this vector has a length of $2^{n}$, like 256, 512, etc.\n",
    "<img src=\"assets/images/06/img_004.png\" width=700 align='center'>\n",
    "\n",
    "If we look at the previous example, translating *comment allez vous* to *how are you*, we can see how the hidden state develops:\n",
    "\n",
    "1. Take the first word and develop the first hidden state:\n",
    "<img src=\"assets/images/06/img_005.png\" width=700 align='center'>\n",
    "\n",
    "2. In the second step, we take the second word AND the first hidden state as inputs to the RNN and produce the second hidden state:\n",
    "<img src=\"assets/images/06/img_006.png\" width=700 align='center'>\n",
    "\n",
    "3. In the third step, we do the same process as the second, we take the third (and last) word AND the second hidden state as inputs and generate the third hidden state:\n",
    "<img src=\"assets/images/06/img_007.png\" width=700 align='center'>\n",
    "\n",
    "The third hidden state is the context vector that will be passed to the decoder. **This highlights a limitation of seq2seq models!**\n",
    "\n",
    "The encoder is confined to sending a single vector, no matter how long or short the input sequence is. Choosing a reasonable size fot this vector makes the model have problems with long input sequences. If you just use a very large number for the hidden unit vectors so that the context is very large, then the model overfits with short sequences and there is a performance reduction as you increase the number of parameters. **Attention in neural nets solves this issue.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424985c0-560f-438f-9213-442f8b2ca8a6",
   "metadata": {},
   "source": [
    "# 2. Attention overview - Encoding\n",
    "\n",
    "A seq2seq model with attention works like this:\n",
    "\n",
    "1. The encoder processes the input sequence, just like the model without attention, one word at a time. It produces a hidden state for each of these inputs and uses that hidden state in the next step.\n",
    "<img src=\"assets/images/06/img_008.png\" width=700 align='center'>\n",
    "\n",
    "2. Then, the model passes the context vector to the decoder. However, unlike the context vector in the model WITHOUT attenttion, this one is not just the final hidden state, it's all of the hidden states.\n",
    "<img src=\"assets/images/06/img_009.png\" width=700 align='center'>\n",
    "\n",
    "The benefit of passing all the hidden input states is that it gives us flexibility in the context size. Longer sequences can have longer context vectors that better capture the information from the input sequence.\n",
    "\n",
    "Intuitively, each hidden state is (likely) most associated with the part of the input sequence that preceded how that word was generated. I.e., the first hidden state was produced after encoding the first word/input so it captures the essence of the first input the most of the hidden states.\n",
    "\n",
    "So, when we **focus** on the first hidden state, we **focus** on the first input. And likewise when we focus on the second hidden state, we are focusing on the second input, and so on.\n",
    "<img src=\"assets/images/06/img_010.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b30a2-e380-4eb3-b1bd-73a6aa895e69",
   "metadata": {},
   "source": [
    "3: Attention Overview - Decoding\n",
    "\n",
    "At every time step, the attention decoder pays attention to the appropriate part of the input sequence using the context vector. The process for the decoder to know which aspects of the input sequence are best to pay attention to is learned during the training phase.\n",
    "\n",
    "<img src=\"assets/images/06/img_011.png\" width=700 align='center'>\n",
    "\n",
    "The process learned is not as simple as going from the first to the last hidden vector; it's not just associating the current hidden vector with the thing to be predicted. It is more sophisticated.\n",
    "\n",
    "If we consider the example of translating a french sentence to an english one. Assume we have a trained transformer model. Let's take the first 4 words of the sentence on the left of the picture:\n",
    "\n",
    "<img src=\"assets/images/06/img_012.png\" width=700 align='center'>\n",
    "\n",
    "If we consider the words in the top portion of the picture, they are pretty well lined up with their french counterpart. But then, note the next few words to translate are \"zone economique europeene\". Something different happens:\n",
    "\n",
    "<img src=\"assets/images/06/img_013.png\" width=700 align='center'>\n",
    "\n",
    "If we consider the darker the lighter shaded blocks to be associated with the words that are **foucsed** on for producing the next word in the statement, you can see that in the three words, \"zone economique europeene\", are not in sequence attended to to produce \"european economic area\". It is not produced as \"area economic european\" as would be in order from the french \"zone economique europeene\". The model was able to learn this representation from the training data set.\n",
    "\n",
    "The model goes on to more or less produce the successive terms with sequentiality:\n",
    "<img src=\"assets/images/06/img_014.png\" width=700 align='center'>\n",
    "\n",
    "This shows how the attention mechanism lets the model focus on the right parts of the sequence at the right time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969b896-65d0-4b36-a863-23c7fecb503b",
   "metadata": {},
   "source": [
    "## Check-on Learning:\n",
    "\n",
    "True or False: A sequence-to-sequence model processes the input sequence all in one step\n",
    "> * False: a seq2seq model works by feeding one element of the input sequence at a time to the encoder\n",
    "\n",
    "What are two limitations of seq2seq models that are solved by attention methods:\n",
    "1. The fixed size of the context matrix passed fro mthe encoder to the decoder is a bottleneck\n",
    "2. The difficulty of encoding long sequences and recalling long-term dependencies\n",
    "\n",
    "How large is the context matrix in an *attention* seq2seq model?\n",
    "> * Depends on the length of the input sequence. Adding attention alters that where the general seq2seq model is of a fixed length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a42ab0b-df85-4aa2-ac1f-2688752ccac8",
   "metadata": {},
   "source": [
    "# 3. Attention overview - Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bcf9de-cac0-483c-8058-7be5652bd76b",
   "metadata": {},
   "source": [
    "In models without attention, we'd only feed the last context vector to the decoder RNN, in addition to the embedding of the end token, and it will begin to generate an element of the output sequence at each time step. \n",
    "\n",
    "<img src=\"assets/images/06/img_015.png\" width=700 align='center'>\n",
    "\n",
    "The case is different in an attention decoder.\n",
    "\n",
    "<img src=\"assets/images/06/img_016.png\" width=700 align='center'>\n",
    "\n",
    "The attention decoder has the ability to look at the inputted words and the decoder's own hidden state:\n",
    "\n",
    "<img src=\"assets/images/06/img_017.png\" width=700 align='center'>\n",
    "\n",
    "and then do the following:\n",
    "1. Use a scoring function to score each hidden state in the context matrix\n",
    "2. then pass those scores into a softmax function so that all values are positive, between zero and one, and all sum to one - these values are how much each vector will be expressed in the attention vector that the decoder will look at before producing an output\n",
    "\n",
    "<img src=\"assets/images/06/img_018.png\" width=700 align='center'>\n",
    "\n",
    "3. Multiply each vector by its softmax score and then summing up those vectors produces an attention context vector - this is a basic weighted sum operation\n",
    "\n",
    "<img src=\"assets/images/06/img_019.png\" width=700 align='center'>\n",
    "\n",
    "Note: The context vector is an important milestone in this process but it is not the end goal\n",
    "\n",
    "4. Now, the decoder has looked at the input word and the attention context vector, which focuses its attention on the appropriate place of the input sequence - it produces a hidden state and it produces the first word in the output sequence\n",
    "\n",
    "<img src=\"assets/images/06/img_020.png\" width=700 align='center'>\n",
    "\n",
    "5. Next, the decoder takes the previous output and hidden states as an input, generates an attention context vector for that time step, which produces a new hidden state for that time step and next word in the output sequence\n",
    "\n",
    "<img src=\"assets/images/06/img_021.png\" width=700 align='center'>\n",
    "\n",
    "6. This continues until the output sequence is completed\n",
    "\n",
    "<img src=\"assets/images/06/img_022.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0307a-7b25-42f8-8c51-c7e04cb4e903",
   "metadata": {},
   "source": [
    "## Check on Learning:\n",
    "\n",
    "In machine learning applications, the encoder and decoder are typically:\n",
    "* Generative Adversarial Networks (GANs)\n",
    "* **Recurrent Neural Networks (Typically vanilla RNN, LSTM, or GRU)**\n",
    "* Mentats\n",
    "\n",
    "What's a more reasonable embedding size for a real-world application?\n",
    "* 4\n",
    "* **200**\n",
    "* 6,000\n",
    "\n",
    "What are the steps that require calculating an attention vector in a seq2seq model with attention?\n",
    "* Every tiem step in the model (both encoder and decoder)\n",
    "* Every time step in the encoder only\n",
    "* **Every time step in the decoder only**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8790080-3dc0-43df-8dee-9541a9e8e10d",
   "metadata": {},
   "source": [
    "# 4. Bahdanau (additive) and Luong (multiplicative) Attention\n",
    "\n",
    "[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) *additive*\n",
    "\n",
    "[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) *multiplicative*\n",
    "\n",
    "Before delving into the details of the scoring functions, we need to make a distinction between the two major types of attention mechanisms: **additive attention** and **multiplicative attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b0650-0e3d-45db-b7db-283fb9466915",
   "metadata": {},
   "source": [
    "## Additive (Bahdanau) Attention \n",
    "\n",
    "The scoring function in Bahdanau (additive) attention: $$e_{ij}=v_{a}^\\top\\text{tanh}\\left ( W_{a}s_{i-1}+U_{a}h_{j} \\right )$$\n",
    "\n",
    "* $h_{j}$ is the hidden state from the encoder\n",
    "* $s_{i-1}$ is the hidden state of the decoder in the previous time step\n",
    "* $U_{a} , W_{a} , v_{a}$ are all weight matrices that are learned during the training process\n",
    "\n",
    "Basically, this is a scoring function that takes the hidden state of the encoder ($h_{j}$), hidden state of the decoder ($s_{i-1}$) and produces a single number for each decoder time step.\n",
    "\n",
    "The scores are then passed into the softmax:\n",
    "$$a_{ij} = \\frac{exp \\left( e_{ij} \\right) }{\\Sigma_{k=1}^{T_{x}} exp\\left(e_{ik}\\right) }$$\n",
    "\n",
    "and then applied to a weighted sum:\n",
    "$$ c_{i} = \\Sigma_{j=1}^{t_{x}} a_{ij}h_{j}$$\n",
    "\n",
    "where we multiply each encoder hidden state by its score and then we sum them, producing our attention context vector\n",
    "\n",
    "<img src=\"assets/images/06/img_023.png\" width=700 align='center'>\n",
    "\n",
    "In this architecture, the encoder is a **bi-directional RNN** and they produce the encoder vector by concatenating the states of these two layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123e341-165d-4679-88f6-d4c16c660ad2",
   "metadata": {},
   "source": [
    "## Multiplicative (Luong) Attention\n",
    "\n",
    "Luong attention built on top of the Bahdanau attention by adding a couple more scoring functions. The architecture is also different in that they used only the hidden states from the top RNN layer in the encoder which allows the encoder and the decoder to both be stacks of RNNs - this has led to some of the premier models that are in production right now.\n",
    "\n",
    "<img src=\"assets/images/06/img_024.png\" width=700 align='center'>\n",
    "\n",
    "There are three scoring functions that we can choose from in multiplicative attention:\n",
    "\n",
    "$$\n",
    "\\text{score}\\left( \\textbf{h}_{t}, \\overline{\\textbf{h}}_{s} \\right)=\n",
    "\\begin{cases}\n",
    "\\textbf{h}_{t}^\\top \\overline{\\textbf{h}}_{s} & \\text{dot}\\\\\n",
    "\\textbf{h}_{t}^\\top \\textbf{W}_{\\textbf{a}} \\overline{\\textbf{h}}_{s} & \\text{general}\\\\\n",
    "\\textbf{v}_{a}^\\top \\text{tanh} \\left( \\textbf{W}_{\\textbf{a}} \\left [\\textbf{h}_{t} ; \\overline{\\textbf{h}}_{s} \\right ] \\right ) & \\text{concat}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The simplest one is the **dot scoring function**. It multimpleis the hidden states of the encoder by the hidden state of the decoder.\n",
    "\n",
    "The **general scoring function** builds on top of the *dot* scoring function by adding a weight matrix between the encoder hidden state and decoder hidden state.\n",
    "\n",
    "The **concat scoring function** is very similar to additive attention in that it adds up the hidden state of the encoder with the hidden state of the decoder $\\left [\\textbf{h}_{t} ; \\overline{\\textbf{h}}_{s} \\right ]$, but is multiplied by weight matrix, $\\textbf{W}_{\\textbf{a}}$, applies a tanh activation, $\\text{tanh}$, and then multiplies is by another weight matrix, $\\textbf{v}_{a}^\\top$. This is a function that we give the hidden state of the decoder at this time step and the hidden states of the encoder at all the time steps, and it will produce a score for each one of them.\n",
    "\n",
    "<br><br>\n",
    "We then do the softmax just as we did before:\n",
    "\n",
    "$$\\textbf{a}_{t}\\left ( s \\right) = \\text{align} \\left( \\textbf{h}_{t} , \\overline{\\textbf{h}}_{s} \\right) = \\frac{\\text{exp} \\left ( \\text{score} \\left ( \\textbf{h}_{t} , \\overline{\\textbf{h}}_{s} \\right ) \\right ) } { \\Sigma_{s'} \\left ( \\text{score} \\left ( \\textbf{h}_{t} , \\overline{\\textbf{h}}_{s'} \\right ) \\right ) } $$\n",
    "\n",
    "and then that would produce c of t, the attention context vector:\n",
    "\n",
    "$$\\tilde{\\textbf{h}}_{t} = \\text{tanh} \\left( \\textbf{W}_{\\textbf{c}} \\left [ \\textbf{c}_{t} ; \\textbf{h}_{t} \\right] \\right )$$\n",
    "\n",
    "and $\\tilde{\\textbf{h}}_{t}$ is the final output of the decoder.\n",
    "\n",
    "With Luong Attention, the following outcome is possible for an English to German Translation:\n",
    "\n",
    "|class | sentence | \n",
    "|:-|-:|\n",
    "|source| Orlando Bloom and Miranda Kerr still love each other|\n",
    "|reference | Orlando Bloom und Miranda Kerr lieben sich noch immer |\n",
    "|best | Orlando Bloom und Miranda Kerr lieben einander noch immer |\n",
    "|seq2seq w/out attention | Orlando Bloom und Lucas Kerr lieben einander noch immer |\n",
    "\n",
    "This is something we can attribute to capturing all of the information *just* in the last hidden state of the encoder. This is one of the powerful things that attention does: it gives the encoder the ability to look at parts of the input sequence, no matter how far back they were in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f228a2-38ce-4e98-a708-998ddc4e6bb7",
   "metadata": {},
   "source": [
    "# 5. Multiplicative Attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54b653-784e-4f7f-99b8-c38d6333dddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_nlp",
   "language": "python",
   "name": "venv_ud_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
