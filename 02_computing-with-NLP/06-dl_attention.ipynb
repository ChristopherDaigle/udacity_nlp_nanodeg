{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194952ff-5c29-46d7-be7f-bbeb2af15a7e",
   "metadata": {},
   "source": [
    "# Introduction to Attention\n",
    "<!-- estimated time: 4hours -->\n",
    "\n",
    "This section will cover:\n",
    "\n",
    "1. Sequence to sequence recap\n",
    "2. Attention overview - Encoding\n",
    "3. Attention overview - Decoding\n",
    "4. Bahdanau and Luong Attention\n",
    "5. Multiplicative attention\n",
    "6. Additive attention\n",
    "7. Computer vision applications\n",
    "8. NLP application: Google neural machine translation\n",
    "9. Other Attention Methods\n",
    "10. The Transformer and Self-Attention\n",
    "* Lab: Attention basics\n",
    "\n",
    "Attention started out in the field of computer vision as an attempt to mimic human perception:\n",
    "> \"One important property of human percetption is that one does not tend to process a while in its entirety at once. Instead, humans focus attention selectively on parts of the visual space to acquire information when and where it is needed, and combine information from different fixations over time to build up an internal representation of the scene, guiding future eye movements and decision making\"\n",
    "- [Recurrent Models of Visual Attention](https://arxiv.org/abs/1406.6247)\n",
    "\n",
    "Note here that instead of processing the entirety of the image, all that is needed to know it is a picture of a bird is to ignore the background and instead focus on the item of interest. Further, if we can separate attention from the entirety of the image to componenets of it, we can describe the image in a more complete and nuanced manner:\n",
    "<img src=\"assets/images/06/img_001.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c63eca-d6d7-48fb-af48-e7e406a86e5c",
   "metadata": {},
   "source": [
    "# 1: Seq2Seq Recap\n",
    "\n",
    "Classic, i.e., those without attention, Seq2Seq models have to look at the original sentence that is to be translated one time and then use that *entire* input to produce every single small output term.\n",
    "\n",
    "A sequence to sequence model takes in an input that is a sequence of items and then produces another sequence of items as an output.\n",
    "\n",
    "* In machine translation, the input sequence is a series of words in one language and the output is a translation in another language.\n",
    "\n",
    "* In text summarization, the input is a long sequence of words and the output is a short one.\n",
    "\n",
    "The seq2seq model usually consists of an encoder and decoder. It works by the encoder first processing all of the inputs, turning the inputs into a single representation. Typically a single vector known as the **context** vector. The *context* vector contains whatever information the encoder was able to capture from the input sequence.\n",
    "<img src=\"assets/images/06/img_002.png\" width=700 align='center'>\n",
    "\n",
    "The context vector is then sent to the decoder which uses it to formulate an output sequence. In machine translation scenarios, the encoder and decoder are both recurrent neural networks (RNNs), usually LSTM cells (long short term memory)\n",
    "<img src=\"assets/images/06/img_003.png\" width=700 align='center'>\n",
    "\n",
    "In this scenario, the context vector is a vector of numbers encoding the information that the encoder captured from the input sequence. In real world scenarios, this vector has a length of $2^{n}$, like 256, 512, etc.\n",
    "<img src=\"assets/images/06/img_004.png\" width=700 align='center'>\n",
    "\n",
    "If we look at the previous example, translating *comment allez vous* to *how are you*, we can see how the hidden state develops:\n",
    "\n",
    "1. Take the first word and develop the first hidden state:\n",
    "<img src=\"assets/images/06/img_005.png\" width=700 align='center'>\n",
    "\n",
    "2. In the second step, we take the second word AND the first hidden state as inputs to the RNN and produce the second hidden state:\n",
    "<img src=\"assets/images/06/img_006.png\" width=700 align='center'>\n",
    "\n",
    "3. In the third step, we do the same process as the second, we take the third (and last) word AND the second hidden state as inputs and generate the third hidden state:\n",
    "<img src=\"assets/images/06/img_007.png\" width=700 align='center'>\n",
    "\n",
    "The third hidden state is the context vector that will be passed to the decoder. **This highlights a limitation of seq2seq models!**\n",
    "\n",
    "The encoder is confined to sending a single vector, no matter how long or short the input sequence is. Choosing a reasonable size fot this vector makes the model have problems with long input sequences. If you just use a very large number for the hidden unit vectors so that the context is very large, then the model overfits with short sequences and there is a performance reduction as you increase the number of parameters. **Attention in neural nets solves this issue.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424985c0-560f-438f-9213-442f8b2ca8a6",
   "metadata": {},
   "source": [
    "# 2. Attention overview - Encoding\n",
    "\n",
    "A seq2seq model with attention works like this:\n",
    "\n",
    "1. The encoder processes the input sequence, just like the model without attention, one word at a time. It produces a hidden state for each of these inputs and uses that hidden state in the next step.\n",
    "<img src=\"assets/images/06/img_008.png\" width=700 align='center'>\n",
    "\n",
    "2. Then, the model passes the context vector to the decoder. However, unlike the context vector in the model WITHOUT attenttion, this one is not just the final hidden state, it's all of the hidden states.\n",
    "<img src=\"assets/images/06/img_009.png\" width=700 align='center'>\n",
    "\n",
    "The benefit of passing all the hidden input states is that it gives us flexibility in the context size. Longer sequences can have longer context vectors that better capture the information from the input sequence.\n",
    "\n",
    "Intuitively, each hidden state is (likely) most associated with the part of the input sequence that preceded how that word was generated. I.e., the first hidden state was produced after encoding the first word/input so it captures the essence of the first input the most of the hidden states.\n",
    "\n",
    "So, when we **focus** on the first hidden state, we **focus** on the first input. And likewise when we focus on the second hidden state, we are focusing on the second input, and so on.\n",
    "<img src=\"assets/images/06/img_010.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b30a2-e380-4eb3-b1bd-73a6aa895e69",
   "metadata": {},
   "source": [
    "3: Attention Overview - Decoding\n",
    "\n",
    "At every time step, the attention decoder pays attention to the appropriate part of the input sequence using the context vector. The process for the decoder to know which aspects of the input sequence are best to pay attention to is learned during the training phase.\n",
    "\n",
    "<img src=\"assets/images/06/img_011.png\" width=700 align='center'>\n",
    "\n",
    "The process learned is not as simple as going from the first to the last hidden vector; it's not just associating the current hidden vector with the thing to be predicted. It is more sophisticated.\n",
    "\n",
    "If we consider the example of translating a french sentence to an english one. Assume we have a trained transformer model. Let's take the first 4 words of the sentence on the left of the picture:\n",
    "\n",
    "<img src=\"assets/images/06/img_012.png\" width=700 align='center'>\n",
    "\n",
    "If we consider the words in the top portion of the picture, they are pretty well lined up with their french counterpart. But then, note the next few words to translate are \"zone economique europeene\". Something different happens:\n",
    "\n",
    "<img src=\"assets/images/06/img_013.png\" width=700 align='center'>\n",
    "\n",
    "If we consider the darker the lighter shaded blocks to be associated with the words that are **foucsed** on for producing the next word in the statement, you can see that in the three words, \"zone economique europeene\", are not in sequence attended to to produce \"european economic area\". It is not produced as \"area economic european\" as would be in order from the french \"zone economique europeene\". The model was able to learn this representation from the training data set.\n",
    "\n",
    "The model goes on to more or less produce the successive terms with sequentiality:\n",
    "<img src=\"assets/images/06/img_014.png\" width=700 align='center'>\n",
    "\n",
    "This shows how the attention mechanism lets the model focus on the right parts of the sequence at the right time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969b896-65d0-4b36-a863-23c7fecb503b",
   "metadata": {},
   "source": [
    "## Check-on Learning:\n",
    "\n",
    "True or False: A sequence-to-sequence model processes the input sequence all in one step\n",
    "> * False: a seq2seq model works by feeding one element of the input sequence at a time to the encoder\n",
    "\n",
    "What are two limitations of seq2seq models that are solved by attention methods:\n",
    "1. The fixed size of the context matrix passed fro mthe encoder to the decoder is a bottleneck\n",
    "2. The difficulty of encoding long sequences and recalling long-term dependencies\n",
    "\n",
    "How large is the context matrix in an *attention* seq2seq model?\n",
    "> * Depends on the length of the input sequence. Adding attention alters that where the general seq2seq model is of a fixed length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a42ab0b-df85-4aa2-ac1f-2688752ccac8",
   "metadata": {},
   "source": [
    "# 3. Attention overview - Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bcf9de-cac0-483c-8058-7be5652bd76b",
   "metadata": {},
   "source": [
    "In models without attention, we'd only feed the last context vector to the decoder RNN, in addition to the embedding of the end token, and it will begin to generate an element of the output sequence at each time step. \n",
    "\n",
    "<img src=\"assets/images/06/img_015.png\" width=700 align='center'>\n",
    "\n",
    "The case is different in an attention decoder.\n",
    "\n",
    "<img src=\"assets/images/06/img_016.png\" width=700 align='center'>\n",
    "\n",
    "The attention decoder has the ability to look at the inputted words and the decoder's own hidden state:\n",
    "\n",
    "<img src=\"assets/images/06/img_017.png\" width=700 align='center'>\n",
    "\n",
    "and then do the following:\n",
    "1. Use a scoring function to score each hidden state in the context matrix\n",
    "2. then pass those scores into a softmax function so that all values are positive, between zero and one, and all sum to one - these values are how much each vector will be expressed in the attention vector that the decoder will look at before producing an output\n",
    "\n",
    "<img src=\"assets/images/06/img_018.png\" width=700 align='center'>\n",
    "\n",
    "3. Multiply each vector by its softmax score and then summing up those vectors produces an attention context vector - this is a basic weighted sum operation\n",
    "\n",
    "<img src=\"assets/images/06/img_019.png\" width=700 align='center'>\n",
    "\n",
    "Note: The context vector is an important milestone in this process but it is not the end goal\n",
    "\n",
    "4. Now, the decoder has looked at the input word and the attention context vector, which focuses its attention on the appropriate place of the input sequence - it produces a hidden state and it produces the first word in the output sequence\n",
    "\n",
    "<img src=\"assets/images/06/img_020.png\" width=700 align='center'>\n",
    "\n",
    "5. Next, the decoder takes the previous output and hidden states as an input, generates an attention context vector for that time step, which produces a new hidden state for that time step and next word in the output sequence\n",
    "\n",
    "<img src=\"assets/images/06/img_021.png\" width=700 align='center'>\n",
    "\n",
    "6. This continues until the output sequence is completed\n",
    "\n",
    "<img src=\"assets/images/06/img_022.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0307a-7b25-42f8-8c51-c7e04cb4e903",
   "metadata": {},
   "source": [
    "## Check on Learning:\n",
    "\n",
    "In machine learning applications, the encoder and decoder are typically:\n",
    "* Generative Adversarial Networks (GANs)\n",
    "* **Recurrent Neural Networks (Typically vanilla RNN, LSTM, or GRU)**\n",
    "* Mentats\n",
    "\n",
    "What's a more reasonable embedding size for a real-world application?\n",
    "* 4\n",
    "* **200**\n",
    "* 6,000\n",
    "\n",
    "What are the steps that require calculating an attention vector in a seq2seq model with attention?\n",
    "* Every tiem step in the model (both encoder and decoder)\n",
    "* Every time step in the encoder only\n",
    "* **Every time step in the decoder only**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8790080-3dc0-43df-8dee-9541a9e8e10d",
   "metadata": {},
   "source": [
    "# 4. Bahdanau (additive) and Luong (multiplicative) Attention\n",
    "\n",
    "[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) *additive*\n",
    "\n",
    "[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) *multiplicative*\n",
    "\n",
    "Before delving into the details of the scoring functions, we need to make a distinction between the two major types of attention mechanisms: **additive attention** and **multiplicative attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b0650-0e3d-45db-b7db-283fb9466915",
   "metadata": {},
   "source": [
    "## Additive (Bahdanau) Attention \n",
    "\n",
    "The scoring function in Bahdanau (additive) attention: $$e_{ij}=v_{a}^\\top\\text{tanh}\\left ( W_{a}s_{i-1}+U_{a}h_{j} \\right )$$\n",
    "\n",
    "* $h_{j}$ is the hidden state from the encoder\n",
    "* $s_{i-1}$ is the hidden state of the decoder in the previous time step\n",
    "* $U_{a} , W_{a} , v_{a}$ are all weight matrices that are learned during the training process\n",
    "\n",
    "Basically, this is a scoring function that takes the hidden state of the encoder ($h_{j}$), hidden state of the decoder ($s_{i-1}$) and produces a single number for each decoder time step.\n",
    "\n",
    "The scores are then passed into the softmax:\n",
    "$$a_{ij} = \\frac{exp \\left( e_{ij} \\right) }{\\Sigma_{k=1}^{T_{x}} exp\\left(e_{ik}\\right) }$$\n",
    "\n",
    "and then applied to a weighted sum:\n",
    "$$ c_{i} = \\Sigma_{j=1}^{t_{x}} a_{ij}h_{j}$$\n",
    "\n",
    "where we multiply each encoder hidden state by its score and then we sum them, producing our attention context vector\n",
    "\n",
    "<img src=\"assets/images/06/img_023.png\" width=700 align='center'>\n",
    "\n",
    "In this architecture, the encoder is a **bi-directional RNN** and they produce the encoder vector by concatenating the states of these two layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123e341-165d-4679-88f6-d4c16c660ad2",
   "metadata": {},
   "source": [
    "## Multiplicative (Luong) Attention\n",
    "\n",
    "Luong attention built on top of the Bahdanau attention by adding a couple more scoring functions. The architecture is also different in that they used only the hidden states from the top RNN layer in the encoder which allows the encoder and the decoder to both be stacks of RNNs - this has led to some of the premier models that are in production right now.\n",
    "\n",
    "<img src=\"assets/images/06/img_024.png\" width=700 align='center'>\n",
    "\n",
    "There are three scoring functions that we can choose from in multiplicative attention:\n",
    "\n",
    "$$\n",
    "\\text{score}\\left( \\textbf{h}_{t}, \\overline{\\textbf{h}}_{s} \\right)=\n",
    "\\begin{cases}\n",
    "\\textbf{h}_{t}^\\top \\overline{\\textbf{h}}_{s} & \\text{dot}\\\\\n",
    "\\textbf{h}_{t}^\\top \\textbf{W}_{\\textbf{a}} \\overline{\\textbf{h}}_{s} & \\text{general}\\\\\n",
    "\\textbf{v}_{a}^\\top \\text{tanh} \\left( \\textbf{W}_{\\textbf{a}} \\left [\\textbf{h}_{t} ; \\overline{\\textbf{h}}_{s} \\right ] \\right ) & \\text{concat}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The simplest one is the **dot scoring function**. It multimpleis the hidden states of the encoder by the hidden state of the decoder.\n",
    "\n",
    "The **general scoring function** builds on top of the *dot* scoring function by adding a weight matrix between the encoder hidden state and decoder hidden state.\n",
    "\n",
    "The **concat scoring function** is very similar to additive attention in that it adds up the hidden state of the encoder with the hidden state of the decoder $\\left [\\textbf{h}_{t} ; \\overline{\\textbf{h}}_{s} \\right ]$, but is multiplied by weight matrix, $\\textbf{W}_{\\textbf{a}}$, applies a tanh activation, $\\text{tanh}$, and then multiplies is by another weight matrix, $\\textbf{v}_{a}^\\top$. This is a function that we give the hidden state of the decoder at this time step and the hidden states of the encoder at all the time steps, and it will produce a score for each one of them.\n",
    "\n",
    "<br><br>\n",
    "We then do the softmax just as we did before:\n",
    "\n",
    "$$\\textbf{a}_{t}\\left ( s \\right) = \\text{align} \\left( \\textbf{h}_{t} , \\overline{\\textbf{h}}_{s} \\right) = \\frac{\\text{exp} \\left ( \\text{score} \\left ( \\textbf{h}_{t} , \\overline{\\textbf{h}}_{s} \\right ) \\right ) } { \\Sigma_{s'} \\left ( \\text{score} \\left ( \\textbf{h}_{t} , \\overline{\\textbf{h}}_{s'} \\right ) \\right ) } $$\n",
    "\n",
    "and then that would produce c of t, the attention context vector:\n",
    "\n",
    "$$\\tilde{\\textbf{h}}_{t} = \\text{tanh} \\left( \\textbf{W}_{\\textbf{c}} \\left [ \\textbf{c}_{t} ; \\textbf{h}_{t} \\right] \\right )$$\n",
    "\n",
    "and $\\tilde{\\textbf{h}}_{t}$ is the final output of the decoder.\n",
    "\n",
    "With Luong Attention, the following outcome is possible for an English to German Translation:\n",
    "\n",
    "|class | sentence | \n",
    "|:-|-:|\n",
    "|source| Orlando Bloom and Miranda Kerr still love each other|\n",
    "|reference | Orlando Bloom und Miranda Kerr lieben sich noch immer |\n",
    "|best | Orlando Bloom und Miranda Kerr lieben einander noch immer |\n",
    "|seq2seq w/out attention | Orlando Bloom und Lucas Kerr lieben einander noch immer |\n",
    "\n",
    "This is something we can attribute to capturing all of the information *just* in the last hidden state of the encoder. This is one of the powerful things that attention does: it gives the encoder the ability to look at parts of the input sequence, no matter how far back they were in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f228a2-38ce-4e98-a708-998ddc4e6bb7",
   "metadata": {},
   "source": [
    "# 5. Multiplicative Attention\n",
    "\n",
    "<!-- https://www.youtube.com/watch?v=1-OwCgrx1eQ&t=12s -->\n",
    "\n",
    "Previously, we looked at how the key concept of attention is to calculate an attetion weight vector, which is used to amplify the signal from the most relevant parts of the input sequence and in the same time, minimize the signal of the least relevant parts.\n",
    "\n",
    "Now, we'll focus on the scoring function that produce the attention weights. An attention scoring function tends to be a function that takes in the hidden state of the decoder and the set of hidden states of the encoder.\n",
    "\n",
    "As this scoring happens at each timestep on the decoder side, we only use the hidden state of the decoder at that, or the previous, timestep in some scoring methods. Given the two inputs, decoder hidden states at decoding step `t` (vector) AND each encoder hidden state for each each encoding step (matrix), the scoring function produces a vector that scores each of these columns.\n",
    "\n",
    "<img src=\"assets/images/06/img_025.png\" width=700 align='center'>\n",
    "\n",
    "Simplest scoring method is to just calculate the dot-product of two inputs. As an example, two vectors:\n",
    "\n",
    "<img src=\"assets/images/06/img_026.png\" width=700 align='center'>\n",
    "\n",
    "The importance of is number however is what is interesting. The dot product of two vectors is geometrically the same as the multiplying the lengths of two vectors bt the cosine of the angle between them. Recall, cosine has the convenient property of equalling one if the angle is zero and it decreases the wider the angle becomes, same 180.\n",
    "\n",
    "<img src=\"assets/images/06/img_027.png\" width=700 align='center'>\n",
    "\n",
    "Intuitively, that means that if two vectors with the same length, the smaller the angle between them, the larger the dot product becomes. And vice versa.\n",
    "\n",
    "<img src=\"assets/images/06/img_028.png\" width=700 align='center'>\n",
    "\n",
    "Generalized though, we have the form for multiplicative attention, **when we assume the encoder and decoder have the same embedding space**:\n",
    "\n",
    "<img src=\"assets/images/06/img_029.png\" width=700 align='center'>\n",
    "\n",
    "An issue with machine translation however is that the encoder and decoder are likely to have separate embedding spaces. In that case, we use a similar scoring method to the above with a slight variation where a weight matrix between the encoder hidden states and decoder hidden state. The weight matrix isa linear transformation that allows the inputs and outputs to use different embedding spaces and the result of this multiplication is the weights vector:\n",
    "\n",
    "<img src=\"assets/images/06/img_030.png\" width=700 align='center'>\n",
    "\n",
    "If we look at things step by step:\n",
    "\n",
    "A: \n",
    "\n",
    "1. The attention decoder (purple outlined blue filled circle) starts by taking an initial hidden state ($\\text{h}_{\\text{init}}$) as well as the embedding for the `<END>` symbol. It does its calculation and generates the hidden state at that time step, ignoring the actual outputs of the RNN, just using the hidden state:\n",
    "\n",
    "<img src=\"assets/images/06/img_031.png\" width=700 align='center'>\n",
    "\n",
    "2. Then we do the attention step (white outline box). We do that by taking in the matrix of hidden states from the encoder (yellow 3x4 matrix). We produce the scoring as we mentioned (pink vector).\n",
    "\n",
    "> If we're doing multiplicative attention, we'll use the dot product.\n",
    "> We'll transform the scores with softmax\n",
    "> Multiply softmax scores by each corresponding hidden state from the encoder\n",
    "\n",
    "3. Sum the scores to produce the attention context vector (blue vector)\n",
    "\n",
    "4. Concatenate the attention context vector ($\\text{C}_{4}$ , blue vector) with the hidden state of the decoder ($\\text{h}_{4}$ , purple vector) at the timestep\n",
    "\n",
    "> In the example, h4 with c4\n",
    "> So, we glue them together as one vector\n",
    "\n",
    "5. Then pass them through a fully connected neural network (pink rounded rectangle) which is basically multiplying by the weights matrix $\\text{W}_{\\text{c}}$ and apply a $tanh$ activation\n",
    "> The output of the fully connected layer would be our *first* outputted word in the output sequence\n",
    "\n",
    "<img src=\"assets/images/06/img_032.png\" width=700 align='center'>\n",
    "\n",
    "Now proceed to second step, B:\n",
    "\n",
    "1. Take the output (pink vector, $\\text{how}^{*}$) from the first decoder timestep (section 4)\n",
    "\n",
    "2. Produce h5 (purple vector) and start the attention (white dashed-line box) at this step\n",
    "> Score, produce weight vector, softmax, multiply\n",
    "\n",
    "3. Sum the scores to produce the context vector at state 5 (blue vector)\n",
    "\n",
    "4. Concatenate h5 and c5\n",
    "\n",
    "5. Then pass them through a fully connected neural network (pink rounded rectangle) which is basically multiplying by the weights matrix $\\text{W}_{\\text{c}}$ and apply a $tanh$ activation\n",
    "> The output of the fully connected layer would be our *second* outputted word in the output sequence\n",
    "\n",
    "<img src=\"assets/images/06/img_033.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35f4b9-f0f1-4dde-a461-837c301f2993",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Additive Attention\n",
    "\n",
    "We will now look at the 3rd commonly used scoring-function. It is called \"concat\" and we will do this through a *feed forward* neural network. The concat scoring method is commonly done by concatenating two vectors and making that the input to a feed forward neural network.\n",
    "\n",
    "As a simple example, let's consider we are scoring the 1st time-step encoder hidden state with a 4th time-step decoder hidden state.\n",
    "\n",
    "1. Merge the vectors to a single vector\n",
    "2. Pass the merged vector to a feedforward neural network\n",
    "> The feedforward neural network has a single hidden layer and outputs the score\n",
    "> There paramters of this network are learned during the training process\n",
    ">> $\\text{W}_{\\text{a}}$ weights matrix and the $\\text{V}_{\\text{a}}$\n",
    "\n",
    "<img src=\"assets/images/06/img_034.png\" width=700 align='center'>\n",
    "\n",
    "The calculation can be viewed as follows:\n",
    "\n",
    "<img src=\"assets/images/06/img_035.png\" width=700 align='center'>\n",
    "\n",
    "Something to note is the difference in concat method from the Bahdanau (additive) and Luong (multiplicative) scoring method:\n",
    "* Multiplicative has one weight matrix\n",
    "* Additive has two major differences\n",
    "> * Weights matrix is split into two: $\\text{W}_{\\text{a}}$ and $\\text{U}_{\\text{a}}$ ; each is applied to the respective vector ($\\text{W}_{\\text{a}}$ to the decoder hidden state and $\\text{U}_{\\text{a}}$ to the encoder hidden state ) <br>\n",
    "> * Use decoder hidden state from the previous time-step\n",
    "\n",
    "<img src=\"assets/images/06/img_036.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07db4f-f651-4743-b93b-cc15c9729452",
   "metadata": {},
   "source": [
    "# 7. Computer Vision Applications\n",
    "\n",
    "Super interesting computer vision applications using attention:\n",
    "* [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)\n",
    "> Achieved SOTA performance in caption generation in a number of datasets\n",
    "> Trained on COCO - Common Objects in Context: set of 200k images with 5 captions each written by people<br>\n",
    "<img src=\"assets/images/06/img_037.png\" width=200 align='left'>\n",
    "<img src=\"assets/images/06/img_038.png\" width=200 align='left'>\n",
    "<img src=\"assets/images/06/img_039.png\" width=200 align='left'>\n",
    "<img src=\"assets/images/06/img_040.png\" width=700 align='center'>\n",
    "<img src=\"assets/images/06/img_041.png\" width=700 align='center'>\n",
    "\n",
    "> * Uses a VGG net trained on ImageNet\n",
    "> * annotations were created from this feature map\n",
    "> * feature volume dimension (14x14x512) meaning 512 features of 14x14 dimension\n",
    "> * Create annotation vector by flattening each feature from 14x14 to 196x1\n",
    ">> * simple reshaping of the matrix to a vector\n",
    "> * Reshaping leads to matrix of 196x512 so that its MxP instead of MxNxP\n",
    ">> * Now have 512 features for each vector of 196 numbers\n",
    "> * This is the context vector and can be used just like we have previously with attention mechanisms\n",
    "<img src=\"assets/images/06/img_042.png\" width=350 align='center'>\n",
    "> * The decoder is an RNN and uses attention to focus on the appropriate annotation vector at each time step\n",
    "> * We plug this into the attention process we've outlined before and that's our image captioning model\n",
    "<img src=\"assets/images/06/img_043.png\" width=700 align='center'>\n",
    "\n",
    "* [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](https://arxiv.org/pdf/1707.07998.pdf)\n",
    ">\n",
    "\n",
    "* [Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S19-04.pdf)\n",
    ">\n",
    "\n",
    "* [Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos](https://arxiv.org/pdf/1507.05738.pdf)\n",
    "\n",
    "* [Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge](https://arxiv.org/pdf/1708.02711.pdf)\n",
    "\n",
    "* [Visual Question Answering: A Survey of Methods and Datasets](https://arxiv.org/pdf/1607.05910.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf1906-c708-435c-9275-4d736f246b30",
   "metadata": {},
   "source": [
    "# 8: NLP application: Google neural machine translation\n",
    "\n",
    "### Google Neural Machine Translation\n",
    "\n",
    "<img src=\"assets/images/06/img_044.jpeg\" width=700 align='center'>\n",
    "\n",
    "The best demonstration of an application is by looking at real-world systems that are in production right now. In late 2016, Google released the following paper describing Google’s Neural Machine Translation System:\n",
    "\n",
    "[Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation [pdf]](https://arxiv.org/pdf/1609.08144.pdf)\n",
    "\n",
    "This system later went into production powering up Google Translate.\n",
    "\n",
    "Take a stab at reading the paper and connecting it to what we've discussed in this lesson so far. Below are a few questions to guide this external reading:\n",
    "\n",
    "* Is the Google’s Neural Machine Translation System a sequence-to-sequence model?\n",
    "* Does the model utilize attention?\n",
    "* If the model does use attention, does it use additive or multiplicative attention?\n",
    "* What kind of RNN cell does the model use?\n",
    "* Does the model use bidirectional RNNs at all?\n",
    "\n",
    "### Text Summarization:\n",
    "\n",
    "[Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond](https://arxiv.org/pdf/1602.06023.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f216d5d-97c2-4b90-9430-b05a21739415",
   "metadata": {},
   "source": [
    "# 9. Other Attention Methods\n",
    "\n",
    "[Paper: Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "[Talk: Attention is all you need attentional neural network models – Łukasz Kaiser](https://www.youtube.com/watch?v=rBCqOTEfxvg)\n",
    "\n",
    "### The Transformer\n",
    "\n",
    "[YouTube](https://www.youtube.com/watch?v=VmsR9FVpQiM&t=1s)\n",
    "\n",
    "Since the two main attention papers were published in 2014 and 2015, attention has been a very active area of research. While the two mechanisms continue to be commonly used, there habe been significant development over the years.\n",
    "\n",
    "The Attention Is All You Need This paper noted that the complexity of the econder-decoder with attention models can be simplified by adopting a new type of model that **only** used attention, no RNNs. This model type is called the **Transformer**. In two of their experiments on machine translation tasks, the model proved superior in quality as well as requiring signifcantly less time to train.\n",
    "\n",
    "The transformer takes a sequence as an input and generates a sequence, just like the seq2seq models we've discussed. The *difference* however is that it does not take the inputs one-by-one as the RNN does. Instead, it can produce all of the together in parallel. Perahps each element is processed by a seperate GPU if we want. It then produces the output one-by-one but also not using an RNN.\n",
    "\n",
    "The Transformer model also breaks down into an encoder and a decoder, but replace RNNs, it uses a feed-forward NN and a concept called *self-attention*. This combination allows the encoder and decoder to work without RNNs, which vastly improves performance since it allows parallelization of processing that was not possible with RNNs.\n",
    "\n",
    "<img src=\"assets/images/06/img_045.png\" width=350 align='center'>\n",
    "\n",
    "The transformer contains a stack of identical encoders and decoders. Six is the number the paper proposes.\n",
    "\n",
    "<img src=\"assets/images/06/img_046.png\" width=350 align='center'>\n",
    "\n",
    "Let's focus on the encoder more closely. Each encoder layer contains two sub-layers - a multiheaded self-attention layer and a feed-forward layer:\n",
    "\n",
    "<img src=\"assets/images/06/img_047.png\" width=350 align='center'>\n",
    "\n",
    "Note that this is contained completely in the encoder instead of being a decoder component like the previous attention mechanisms. This attention component helps the encoder comprehend its inputs by focusing on other parts of the input sequence that are relevant to each input element it processes. This idea is an extension of the work previously done on the concept of self-attention and it can aid comprehension.\n",
    "\n",
    "In other architectures, like the LSTM, you can see an example of an implementation of a self-attention mechanism: \n",
    "\n",
    "<img src=\"assets/images/06/img_048.png\" width=350 align='center'>\n",
    "\n",
    "The structure of the transformer however allows for the model to focus on tokens that appear before AND after the token of interest instead of only the preceding tokens.\n",
    "\n",
    "This is not the only attention mechanism in the transformer architecture however. The decoder contains an *encoder-decoder* attention mechanism and a *sel-attention* mechanism. One that allows it to focs on the relevant part of the inputs (encoder-decoder attention) and another that only pays attention to previous decoder outputs (self-attention).\n",
    "\n",
    "<img src=\"assets/images/06/img_049.png\" width=350 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60fa28-a0f2-43e8-8a73-015e41c5d4d2",
   "metadata": {},
   "source": [
    "# 10: The Transformer and Self-Attention\n",
    "\n",
    "Say we have words we want our encoder to read and create a representation of. As always, we begin by embedding them into vectors. Since the transformer gives us a lot of flexibility for parallelization, this example assumes we're looking at the process or GPU tasked with encoding the second token of the input sequence.\n",
    "\n",
    "<img src=\"assets/images/06/img_050.png\" width=350 align='center'>\n",
    "\n",
    "The first step is to compare them. We score the embeddings against each other. So we compare the token we are currently \"reading\" or encoding with the other words in the input sequence (i.e., all the words in the input, not just two).\n",
    "\n",
    "<img src=\"assets/images/06/img_051.png\" width=350 align='center'>\n",
    "\n",
    "Then, we scale the score by the dimension of the keys, which we're using a toy dimension of four, so we use $\\sqrt{4}=2$:\n",
    "\n",
    "<img src=\"assets/images/06/img_052.png\" width=350 align='center'>\n",
    "\n",
    "Then, we perform softmax on the scores:\n",
    "\n",
    "<img src=\"assets/images/06/img_053.png\" width=350 align='center'>\n",
    "\n",
    "Then we multiply the softmax score with the embedding to get the level of expression of each of these vectors:\n",
    "\n",
    "<img src=\"assets/images/06/img_054.png\" width=350 align='center'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d5711-e346-42cd-9e42-f08b5a9268d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_nlp",
   "language": "python",
   "name": "venv_ud_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
