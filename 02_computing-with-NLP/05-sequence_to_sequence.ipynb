{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4fdbdd-4400-4e46-a260-63265cb54253",
   "metadata": {},
   "source": [
    "# Sequence to Sequence (Seq2Seq)\n",
    "\n",
    "Hello! For this next section, I'd like to introduce you to Jay Alammar. Jay has done some great work in interactive explorations of neural networks. If you haven't already, make sure you check out [his blog.](http://jalammar.github.io/)\n",
    "\n",
    "Jay will be teaching you about a particular RNN architecture called \"sequence to sequence\". In this case, you feed in a sequence of data and the network will output another sequence. This is typically used in problems such as machine translation, where you'd feed in a sentence in English and get out a sentence in Arabic.\n",
    "\n",
    "Sequence to sequence will prepare you for the next section, **Deep Learning Attention**, also taught by Jay.\n",
    "\n",
    "## Lesson Outline\n",
    "1. Applications\n",
    "2. Architectures\n",
    "3. Architectures in More Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02410fc6-8143-4042-8164-520d71ddf87b",
   "metadata": {},
   "source": [
    "# <a id='0'>0: Introduction</a>\n",
    "\n",
    "We've known that we can do simple sentiment analysis using normal feedforward neural networks the network is able to learn how positive or negative each word is and can if a sequence as a whole has positive or negative things to say about its subject. However, we start running issues when we want to do a little bit more advanced models that deal with language and sequential data. Let's look at an example in the video.\n",
    "\n",
    "<img src=\"assets/images/05/img_001.png\" width=700 align='center'>\n",
    "\n",
    "Given the two sentences, if we wanted to find the year:\n",
    "> I went to Nepal in **2009**<br>\n",
    "> In **2009**, I went to Nepal\n",
    "\n",
    "If we used a typical feedforward network, it would have to have separate parameters for each input feature (i.e., every word). Technically, it would have to learn all the rules of language separately at each position in the input sentence.\n",
    "\n",
    "Recurrent nets are a powerful class of neural nets that deal with sequential data. They are especially suited for language and translation tasks because they can extend to sequences of any length. More importantly, they share parameters across different time steps. When they do learn a language model, they do it more efficiently than a traditional feed-forward network would.\n",
    "\n",
    "<img src=\"assets/images/05/img_002.png\" width=700 align='center'>\n",
    "\n",
    "[Source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "**Sequential data** can refer to the input to the model, the output of the model, or both. The above image demonstrates different kinds of RNNs that are suited for different types of tasks.\n",
    "\n",
    "* many-to-one: reads a **sequence** of words and outputs a single value<br>\n",
    "> input: sequential<br>\n",
    "> output: singular<br>\n",
    "* many-to-many: reads a sequence of words and outputs a sequence of words<br>\n",
    "> input: sequential<br>\n",
    "> output: sequential<br>\n",
    "\n",
    "In the second *many-to-many* option, we are using a single RNN where we are forced to ouput at most as many vectors as we input. But that wouldn't work well for a chatbot where we would like the outcome to be unlimited in the length of items it returns. We want the model to take in the entire input before we start generating a response.\n",
    "\n",
    "In the first *many-to-many* option, it is composed of two RNNs that can map a sequence of any length to another sequence of any length. The basic premise is that we use two RNNs, an input (encoder network) and an output (decoder network), where the first reads the input sequence and the second generates the output sequence. It does this by the encoder handing what it learned to the decoder network.\n",
    "\n",
    "<img src=\"assets/images/05/img_003.png\" width=700 align='center'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98b1a6-3dc0-4953-bb92-43efb703ea2e",
   "metadata": {},
   "source": [
    "# 1: Application\n",
    "\n",
    "The term, \"sequence to sequence\", may seem abstract and not clearly demonstrate what can be done with this kind of architecture. So, here are some rudimentary examples of what it means to take in a any sequence and produce any another sequence (if it can be represented as a vector, it can be used in a seq2seq model):\n",
    "\n",
    "1. **Translation Model**\n",
    "> input: english phrases<br>\n",
    "> target: french phrases<br>\n",
    "2. **Summarization Model**:\n",
    "> input: news articles<br>\n",
    "> target: summary<br>\n",
    "3. **Question and Answering Model**\n",
    "> input: questions<br>\n",
    "> target: answers<br>\n",
    "4. **Chatbot**\n",
    "> input: transmission dialog<br>\n",
    "> target: response dialog<br>\n",
    "\n",
    "But inputs don't have to only be words. The RNNs are used along convolutional nets. They can be images or audio.\n",
    "\n",
    "<img src=\"assets/images/05/img_004.png\" width=700 align='center'>\n",
    "\n",
    "There are many options. The biggest challenge is to find the right data set to build what you are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb23a60-37b5-4e38-8d08-01825e418de3",
   "metadata": {},
   "source": [
    "# 2: Architectures\n",
    "\n",
    "Recall the basic 2 RNN architecture of encoder and decoder:\n",
    "<img src=\"assets/images/05/img_005.png\" width=700 align='center'>\n",
    "\n",
    "The first RNN, encoder, reads the input sequence and hands over what it has understood to the second RNN, decoder, which generates the output sequence.\n",
    "\n",
    "The \"understanding\" that the encoder \"hands over\" is a fixed size tensor that is called a \"state\" or \"context\". No matter how long the inputs and outputs are, the context remains the same size as when you built the model.\n",
    "\n",
    "At a high level, the inference process is done by handing **inputs** to the **encoder**, the encoder summarizes what it understood into a state or context variable, it hands it over to the decoder, which then generates the output sequence.\n",
    "\n",
    "Going a level deeper:\n",
    "Since the encoder and decoder are both RNNs, they have **loops** which allows them to process these sequences of inputs and outputs.\n",
    "\n",
    "<img src=\"assets/images/05/img_006.png\" width=700 align='center'>\n",
    "\n",
    "As an example, think of a chatbot:\n",
    "\n",
    "We want to ask it *How are you?*. First we tokenize the input into four elements$\\left[\\text{How,are,you,?}\\right]$. Because there are 4 elements, it will take any RNN 4 time steps (loops) to read in the entire sequence. At each loop, it reads the input, does a transformation on the hidden state, and pass the hidden state out to the next time step. In the diagram, the clock symbol indicates we are moving from one time step to the next.\n",
    "\n",
    "<img src=\"assets/images/05/img_007.png\" width=700 align='center'>\n",
    "\n",
    "If we think about the RNN as a loop, we can also \"unroll\" the RNN and show the steps sequentially laid out in a line with blocks representing the hidden state. A note, the blocks are actually only ONE block, but it gets updated to a new \"state\" through transformation at each time step with the new input and the previous time step:\n",
    "\n",
    "<img src=\"assets/images/05/img_008.png\" width=700 align='center'>\n",
    "\n",
    "So, what is the **hidden state**? The hidden state is a number hidden units inside of the cell. In practice, its most likely to be the hidden state inside an LSTM (long short term memory) cell. The size of the network is another hyperparameter that can be set. Generally, the bigger the hidden state, thus the bigger the size, the more capacity of the model to observe and learn patterns. However, the large the network, the more resources are needed to train and deploy such a model.\n",
    "\n",
    "<img src=\"assets/images/05/img_009.png\" width=700 align='center'>\n",
    "\n",
    "Similar things happen with the decoder. We begin by \"feeding\" the decoder the state (or context) generated by the encoder, then it generates the output value(s) element by element.\n",
    "\n",
    "<img src=\"assets/images/05/img_010.png\" width=700 align='center'>\n",
    "\n",
    "If we unroll it like we did the encoder, we see that we are feeding it back every element that it outputs. This helps to create coherent outcomes as each previous time step informs the succeeding output. It's as though the current time step *remembers* what the previous time step has emitted.\n",
    "\n",
    "<img src=\"assets/images/05/img_011.png\" width=700 align='center'>\n",
    "\n",
    "Connected all of that together, we can see something of the sort:\n",
    "\n",
    "<img src=\"assets/images/05/img_012.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652fa67-b185-4cc9-a64d-333fe9674926",
   "metadata": {},
   "source": [
    "# 3. Architecture in More Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7163f1-7c4a-4449-ae2d-3659ab6e3f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_keras",
   "language": "python",
   "name": "venv_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
