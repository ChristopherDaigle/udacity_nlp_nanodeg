{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1330dce2-8a38-4cd1-a49a-5268d9a84289",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "[1. Bag-of-Words in More Detail](#1)<br>\n",
    "[2. Latent Variables](#2)<br>\n",
    "[3. Matrix Representation of Latent Dirichlet Allocation](#3)<br>\n",
    "\n",
    "> [3.1: Picking Topics](#3.1)<br>\n",
    "\n",
    "[4. Beta Distributions](#4)<br>\n",
    "[5. Dirichlet Distribution](#5)<br>\n",
    "[6. More on Latent Dirichlet Allocation](#6)<br>\n",
    "[7. Sample a Topic](#7)<br>\n",
    "[8. Sample a Word](#8)<br>\n",
    "[9. Combining the Models](#9)<br>\n",
    "\n",
    "> [9.1 Summary](#9.1)<br>\n",
    "\n",
    "[10. Topic Modeling Lab](#10)<br>\n",
    "\n",
    "## References\n",
    "In this section, we'll be following this article by David Blei, Andrew Ng, and Michael Jordan.\n",
    "* [Latent Dirichlet Allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485017ec-d60f-4687-a9d3-4f64317e4375",
   "metadata": {},
   "source": [
    "# <a id='1'>1: Bag-of-Words (BoW) in More Detail</a>\n",
    "\n",
    "If you think about the BoW model graphically, it represents the relationship between a set of document objects and a set of word objects.\n",
    "\n",
    "Assume we have the article, *Space exploration. A vote to explore space has been explored*, and that we have done a good job processing the text (case, stemming, lemmatization, etc.)\n",
    "* There are three main terms: **space**, **vote**, and **explore**\n",
    "* To find the probability of each term appearing in the article, we divide the count of each term by the total number of terms\n",
    "* We have three parameters - probabilities for each term ( $p(\\text{space|article})$, $p(\\text{vote|article})$, $p(\\text{explore|article})$ )\n",
    "\n",
    "To add some notation:\n",
    "* d: documents (units of groups of terms to be analyzed)\n",
    "* t: terms (elements that compose documents)\n",
    "* P(t|d): probability of a term appearing in the document (\"For any given document, $d$, and observed term, $t$, how likely is it that the document $d$ generated the term $t$\")\n",
    "\n",
    "<img src=\"assets/images/03/img_01.png\" width=700 align='center'>\n",
    "\n",
    "Now, if we do this for many documents, say 500, and many terms, say 1,000, we can get something of the sort:\n",
    "\n",
    "<img src=\"assets/images/03/img_02.png\" width=700 align='center'>\n",
    "\n",
    "If we have 500,000 parameters, that is a lot of parameters to figure out. We can reduce the number of parameters and still keep most of the information by representing the terms in a latent space. This is commonly known as **topic modeling**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6962a2-ca7e-46fc-a84c-1320a233c2e7",
   "metadata": {},
   "source": [
    "# <a id='2'>2: Latent Variables</a>\n",
    "\n",
    "Consider adding to the model the notion of a small set of topics or latent variables (or themes) that actually drive the generation of words in each document. So in this model, any document is considered to have an underlying mixture of topics associated with it. Similarly, a topic is considered to be a mixture of terms that it is likely to generate.\n",
    "\n",
    "If we take our **documents**, our **terms**, and assert there are a number of **topics**, say 3, then we have 2-sets of probability distributions:\n",
    "1. $p(\\text{z|d})$: topic-document probability (probability of topic $z$ given a document $d$)\n",
    "2. $p(\\text{t|z})$: term-topic probability (probability of a term $t$ given a topic $z$)\n",
    "\n",
    "Our new probability of a document given a term, $p(\\text{t|d})$, can be expressed as a sum over the two previous probabilities:\n",
    "$$P\\left( t \\mid d \\right) = \\Sigma_{z} P\\left( t \\mid z \\right) \\cdot P\\left( z \\mid d \\right)$$\n",
    "\n",
    "<img src=\"assets/images/03/img_03.png\" width=700 align='center'>\n",
    "\n",
    "Now, the number of parameters is: (number of documents * number of topics) + (number of topics * number of terms)\n",
    "* 500 documents, 10 topics, 1,000 terms: (500 * 10) + (10 * 1,000) = 15,000\n",
    "> * Note: same number of documents and terms as before, but much less parameters than 500,000!\n",
    "\n",
    "This is called **Latent Dirichlet Allocation** or LDiA for short."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29949ccf-61fe-4b2e-b170-fe53b4d98a70",
   "metadata": {},
   "source": [
    "# <a id='3'>3: Matrix Representation of Latent Dirichlet Allocation</a>\n",
    "\n",
    "An LDiA is an example of matrix factorization\n",
    "\n",
    "The idea is as follows:<br>\n",
    "<img src=\"assets/images/03/img_04.png\" width=700 align='center'>\n",
    "\n",
    "* We go from a BoW model to an LDiA model\n",
    "> * The BoW on the left basically says \"our probability of, say, the word 'tax' being generated by the second document is the label of the white arrow\"\n",
    "> * The LDiA on the right, that probability is calculated by the white arrows multiplying the probability of a term $t$, say 'tax' in a topic $z$ say 'politics', by the corresponding probability of a topic $z$ given a document $d$ and adding them\n",
    "\n",
    "Then, you can have a BoW matrix, composed of terms as columns and documents as rows, like on the bottom left, equal to, or represented by, the product of two matrices:\n",
    "1. tall skinny matrix of documents as rows and topics as columns\n",
    "2. wide flat matrix of topics as rows and terms as columns\n",
    "\n",
    "In this case, the entry of the second document for the term tax, will be equal to the inner product of the corresponding row and column in the matrices on the right\n",
    "> * If the matrices are big, say 500 documents and 1,000 terms, such that the BoW matrix is 500,000 elements large (500 by 1,000 = $\\text{m} x \\text{n}$)\n",
    "> * The two matrices in the topic model combined have only 15,000 elements (mxn * nxm = (500x10) * (10x1,000) = matrix of size 500 by 1,000 for the original matrix)\n",
    "\n",
    "Aside from being much simpler, the LDiA model has a huge advantage that it gives us a bunch of topics that we can divide documents upon. In this example, we are asserting they are *science, politics, and sports*, but in reality the algorithm will just throw some topics and it'll be up to us to look at the associated words and decide what is the common topic of all these words.\n",
    "\n",
    "For these examples, we'll keep asserting these 3 topics, but think of them instead as *topic 1, topic 2, and topic 3*\n",
    "\n",
    "The LDiA model is represented, as before, as:\n",
    "$$P\\left( t \\mid d \\right) = \\Sigma_{z} P\\left( t \\mid z \\right) \\cdot P\\left( z \\mid d \\right)$$\n",
    "\n",
    "## Matrix Multiplication\n",
    "\n",
    "The idea for building our LDA model will be to factor our BoW matrix into two matrices, one with documents by topic and the other as topic by terms\n",
    "\n",
    "<img src=\"assets/images/03/img_05.png\" width=700 align='center'>\n",
    "\n",
    "Recall how we built our BoW matrix: identify the terms and the number of times they appear in a specific document and divide by the sum of terms in that document to get the probabilities/frequencies:\n",
    "\n",
    "<img src=\"assets/images/03/img_06.png\" width=700 align='center'>\n",
    "\n",
    "For our **document topic matrix**, we have as follows.\n",
    "\n",
    "If we have a document, say `document 3` (or doc 3), and doc 3 is mostly about science and a bit about sports and politics. Maybe it's 70% about science, 10% about politics, and 20% about sports. We record these values in the **document-topic matrix**:\n",
    "\n",
    "<img src=\"assets/images/03/img_07.png\" width=700 align='center'>\n",
    "\n",
    "For the **topic-term matrix**, we have a similar approach. Start with a topic, say politics, and let's say we can figure out the probabilities that words are generated by this topic. We take all these probabilities to sum to one. We take these probabilities and place them into the **topic-term matrix** as such:\n",
    "\n",
    "<img src=\"assets/images/03/img_08.png\" width=700 align='center'>\n",
    "\n",
    "From these two matrices, the product of them together will approximate the BoW matrix!\n",
    "\n",
    "<img src=\"assets/images/03/img_09.png\" width=700 align='center'>\n",
    "\n",
    "But we haven't gone into depth about HOW to calculate the entries in these matrices. One way is to use the traditional [*matrix factorization* algorithm](https://developers.google.com/machine-learning/recommendation/collaborative/matrix). However, these matrices are unique in that each of the rows sum to one and there is a significantly meaningful amount of structure coming from a set of documents, topics and words.\n",
    "\n",
    "What we'll do is something more elaborate than matrix multiplication. The basic idea is that the entries in the two topic modeling matrices come from special distributions. So, we'll embrace this fact and work with these distributions to find these two matrices!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ecd4e0-0c68-44cd-9ac7-c875520bc87c",
   "metadata": {},
   "source": [
    "## <a id='3.1'>3.1: Picking topics</a>\n",
    "\n",
    "Pretend you are at a party in a triangular room. There are people roaming around the room. In each of the corners, there are different things happening. In one corner, there is food, in another corner there is desert, and in the last there is music.\n",
    "\n",
    "<img src=\"assets/images/03/img_10.png\" width=700 align='center'>\n",
    "\n",
    "People naturally get drawn to these corners based on their preferences if they like food, desert, or music. Or perhaps they are undecided and equally space themselves from say food and desert. However, they mostly walk away from the blue areas and toward the red areas.\n",
    "\n",
    "<img src=\"assets/images/03/img_11.png\" width=700 align='center'>\n",
    "\n",
    "Now, imagine the alternative. We are still at a party, but now in the corners, there is a lion, fire, and radioactive material. \n",
    "\n",
    "<img src=\"assets/images/03/img_12.png\" width=700 align='center'>\n",
    "\n",
    "Now, people will do the opposite of what they did when there were desirable things in the corners; they will move away from the corners. They will gravitate toward the center.\n",
    "\n",
    "<img src=\"assets/images/03/img_13.png\" width=700 align='center'>\n",
    "\n",
    "So now, we have three scenarios:\n",
    "1. We place nice things in the corners\n",
    "2. We put nothing in the corners\n",
    "3. We place bad things in the corners\n",
    "\n",
    "<img src=\"assets/images/03/img_14.png\" width=700 align='center'>\n",
    "\n",
    "In the above three scenarios, we can think of the parameters at the corners as *repelling factors*: if they are large, then the points are pushed away, small they draw the points to them, and $1$ the points are static\n",
    "\n",
    "As an example, if we have the following three Dirichlet Distributions, which of these three wis more likely to generate the topics in our model?\n",
    "\n",
    "<img src=\"assets/images/03/img_15.png\" width=700 align='center'>\n",
    "\n",
    "Answer: **Left**\n",
    "> * If we randomly select any point in the distribution, it is most likely, of the three distributions, to be associated strongly with one of the three topics<br>\n",
    "> * Most articles will be represented by one topic strongly, but maybe others weakly, this means then that the distribution that helps us do that, distinguish well the primary topic of an article, is one that is most useful!\n",
    "\n",
    "<img src=\"assets/images/03/img_16.png\" width=700 align='center'>\n",
    "\n",
    "So, for our LDiA model, we will pick a Dirichlet Distribution with small parameters $\\alpha$, such as $\\overrightarrow{\\alpha}=\\{0.7,0.7,0.7\\}$, and from here we'll sample a few points to be our documents. Each point gives us a mixture of probabilities $\\overrightarrow{\\theta}$ that will characterize the distribution of topics for that particular document.\n",
    "\n",
    "<img src=\"assets/images/03/img_17.png\" width=700 align='center'>\n",
    "\n",
    "In 3D, the dirichlet distributions look as such:\n",
    "\n",
    "<img src=\"assets/images/03/img_18.png\" width=700 align='center'>\n",
    "\n",
    "This shows that the probability of picking a point on the triangle depends on the height of the probability distribution at that point. So, as we can see on the left that the edges where the topics are most strong are the highest point on the distribution, we would prefer the one on the left!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e2293-a70a-4b8e-bce3-24b20d27eb72",
   "metadata": {},
   "source": [
    "# <a id='4'>4: $\\beta$ Distributions</a>\n",
    "\n",
    "Let's think about probability distributions.\n",
    "\n",
    "Assume we have a coin and we toss it twice. The outcomes are 1 heads and 1 tails. What do we think about this coin? It could be a fair coin, it could be biased toward heads or tails, but we don't have enough data to be sure. To conintue the thought experiment, let's say that we think it's fair, but not with much confidence.\n",
    "\n",
    "So, the probability distribution could look something like this - higher at $\\frac{1}{2}$ but a bit *even* over the entire interval:\n",
    "\n",
    "<img src=\"assets/images/03/img_19.png\" width=700 align='center'>\n",
    "\n",
    "Now, let's say we toss the coint 20 times and we get 10 heads and 10 tails. We feel more confident that the coin is fair. The probability distribution may look something more like this:\n",
    "\n",
    "<img src=\"assets/images/03/img_20.png\" width=700 align='center'>\n",
    "\n",
    "But what if we toss the coin 4 times and get heads 3 times and tails once? We get an average of $\\frac{3}{4}$ on probability of getting heads, but we don't have much confidence. We might have a distribution like such:\n",
    "\n",
    "<img src=\"assets/images/03/img_21.png\" width=700 align='center'>\n",
    "\n",
    "But if we toss it 400 times and get 300 heads and 100 tails, we become more confident in the coin being biased toward heads and may get a probability distribution like this:\n",
    "\n",
    "<img src=\"assets/images/03/img_22.png\" width=700 align='center'>\n",
    "\n",
    "This is called the **$\\beta$ - Distribution** and it works for any values $a$ and $b$:\n",
    "\n",
    "<img src=\"assets/images/03/img_23.png\" width=700 align='center'>\n",
    "\n",
    "The **gamma function** can be thought of as a continuous version of the factorial function.\n",
    "\n",
    "$$\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}x^{a-1}y^{b-1}$$\n",
    "$$\\text{s.t.}$$\n",
    "$$\\Gamma(a)=(a-1)!$$\n",
    "\n",
    "Where $a$ is an integer, we get the general factorial form. But, if $a$ is not an integer, but instead some form of flow, we can get something of the sort:\n",
    "\n",
    "<img src=\"assets/images/03/img_24.png\" width=700 align='center'>\n",
    "\n",
    "So, if we have something like $0.1$ for heads and $0.4$ for tails, aside from it making no sense, we can still use this in the $\\beta$-distribution. We just need to use the right funtion for the probability distribution. The probability distribution, or $\\beta$-distribution looks as such and means that $p$ is much more likely to be close to zero or close to one than to be somewhere in the middle.\n",
    "\n",
    "<img src=\"assets/images/03/img_25.png\" width=700 align='center'>\n",
    "\n",
    "This makes a bit of sense: if $p$ is close to zero or 1, then we are likely to have zero heads or zero tails, which at least gets us close to one of the values we mentioned of $0.1$ or $0.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea1cdd9-dbc6-4e4f-a36f-93945d461a36",
   "metadata": {},
   "source": [
    "# <a id='5'>5: Dirichlet Distribution</a>\n",
    "\n",
    "A multinomial distribution is simply a generalization of the binomial distribution to more than two values. The Dirichlet Distribution is an example of a multinomial distribution.\n",
    "\n",
    "As an example, let's say we have newspaper articles and three topics (just as our earlier examples): science, politics, and sports. Now, let's say that a topic is assigned randomly to the articles and when we look, we find we have: 3 science articles, 6 politics articles, and 2 sports articles. So, for a new article, what is the probability that the article is science, sports, or politics? $\\frac{3}{11}$ probability that it is science, $\\frac{6}{11}$ probability that it is politics, and $\\frac{2}{11}$ probability that it is sports.\n",
    "\n",
    "If we think of these articles as being represented over a probability distribution, we can think of a triangle, as we did previously. Where if the probability of a topic is 1, then it is at the corner, and something less than one, then the point can be on the plane in a few spaces. The first would be along an edge, indicating some probability between two of the topics, and zero at the third. And if the point is somewhere away from the boundary, it would indicate some positive probability among all of the above topics.\n",
    "\n",
    "So, if we represented like the triangle discussed, then the we expect a density at the politics points, then maybe a little less in the science point, and finally the least in the sports point. This distribution is calculated with a generalization of the formula for the $\\beta$ distribution: $\\frac{\\Gamma(a+b+c)}{\\Gamma(a)\\Gamma(b)\\Gamma(c)}x^{a-1}y^{b-1}z^{c-1}$. This generalization is the **Dirichlet Distribution**.\n",
    "\n",
    "<img src=\"assets/images/03/img_26.png\" width=700 align='center'>\n",
    "\n",
    "Now, there is no need for these values to be integers (just as we saw before). For example, if we have 0.7 for each science, politics, and sports, here is an example of the Dirichlet Distribution. It may be difficult to see, but the density function is very high when we get close to the corners of the triangle. Meaning that any point picked randmly from this distribution is very likely to fall either at the science, politics, or sports corners, or at least close to any of the edges. Also, it's very unlikely to be somewhere in the middle.\n",
    "\n",
    "<img src=\"assets/images/03/img_27.png\" width=700 align='center'>\n",
    "\n",
    "Here are some samples of Dirichlet Distributions with different values. Notice that when the values are large, the density function is higher in the middle and if they're small, it's higher in the corners.\n",
    "\n",
    "<img src=\"assets/images/03/img_28.png\" width=700 align='center'>\n",
    "\n",
    "If the values are different than each other, then the high part moves towards the smaller values and away from the larger values.\n",
    "\n",
    "<img src=\"assets/images/03/img_29.png\" width=700 align='center'>\n",
    "\n",
    "Again, this is how they would look in 3D. So notice that if we want a good topic model, we need to pick small parameters like the one on the left:\n",
    "\n",
    "<img src=\"assets/images/03/img_18.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84fbc7c-423c-4f23-a84c-9ea6146f7eb5",
   "metadata": {},
   "source": [
    "# <a id='6'>6: More on Latent Dirichlet Allocation</a>\n",
    "\n",
    "So now, let's build our LDiA model.\n",
    "\n",
    "Let's say we have 3 real documents and then we'll generate 3 fake documents. The way we create the fake documents is with a **topic model**. Then, after we generated them, we compare the generated documents with the real ones.\n",
    "\n",
    "By comparing the generated documents with the real ones, this will tell us how far we are from creating the real documents with our model. As with most machine learning algorithms, we learned from these errors and we'll be able to improve the topic model.\n",
    "\n",
    "<img src=\"assets/images/03/img_30.png\" width=700 align='center'>\n",
    "\n",
    "In the paper referenced earlier, [Latent Dirichlet Allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), the topic model is drawn as such and appears very complicated. But we will continue to analyze this in our following sections:\n",
    "\n",
    "<img src=\"assets/images/03/img_31.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a0154-6ce0-488a-97f1-5e519b2cadf5",
   "metadata": {},
   "source": [
    "# <a id='7'>7: Sample a Topic</a> \n",
    "\n",
    "Let's start by picking some topics for our documents. We start with some Dirichlet Distribution with paramters $\\alpha$. The parameters should be small so that the distribution is *spiky* toward the points. So that if we pick a point somewhere in the distribution, it will most likely be close to a corner or at least an edge. \n",
    "\n",
    "Let's pick a point close to the politics corner which generates the following values: 0.1 science, 0.1 sports, 0.8 for politics. These values represent a mixture of topics for this particular document. They also give us a multinomial distribution $\\theta$. Now, from this distribution, we'll start picking topics. That means the topics we'll pick are Science with 10% probability, Sports with 10% probability, and Politics with 80% probability.\n",
    "\n",
    "<img src=\"assets/images/03/img_32.png\" width=700 align='center'>\n",
    "\n",
    "Now, we do this with several documents. So, each document is a point in this declared distribution. We can see that each document can be represented along the distribution with a value for each point indicating how close one is from any one topic.\n",
    "\n",
    "<img src=\"assets/images/03/img_33.png\" width=700 align='center'>\n",
    "\n",
    "Now, we can merge all of these vectors to get the first matrix, the matrix that indexes documents with the their corresponding topics, the **document-topic matrix**.\n",
    "\n",
    "<img src=\"assets/images/03/img_34.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a1bb4-2f24-4788-acdb-7ef212a56fc1",
   "metadata": {},
   "source": [
    "# <a id='8'>8: Sample a Word</a>\n",
    "\n",
    "Now, we'll do the same thing for topics and words. Let's say, for the sake of visualization, we only have four words: space, climate, vote, and rule. Now, these 4 words give us a different Dirichlet Distribution, $\\beta$. This one is similar, but it is 3D. It is not around a triangle, but it is around a simplex. The red and blue parts are high and low probability areas respectively. If we had more words, we would still have a Dirichlet Distribution, except it would be in a much higher dimensional simplex.\n",
    "\n",
    "So, in this distribution $\\beta$ we pick a random point and it will very likely be close to a corner or an edge. Each point chose in the distribution will have a probability associated with each word. For our example, we can assume we get: 0.4 space, 0.4 climate, 0.1 vote, and 0.1 rule. This multinomial distribution is phi\" $\\phi$ and it represents the *connection between the words and the topic*. Now, from this distribution, we'll sample random words (which are 40% likely to be space, 40% likely to be climate, 10% likely to be vote, and 10% likely to be rule).\n",
    "\n",
    "<img src=\"assets/images/03/img_35.png\" width=700 align='center'>\n",
    "\n",
    "So, now we do this for **every topic**. Notice that with this, we have topics by number. We do not know them by any name, we just know them by topic 1, 2, and 3. After some inspection, we can infer that topic 1, being close to space and climate must be science. Similarly, topic 2 being close to vote could be politics, and topic 3 close to rule could be sports. This inference of topic meaning from inspection is something you do at the end of the model.\n",
    "\n",
    "<img src=\"assets/images/03/img_36.png\" width=700 align='center'>\n",
    "\n",
    "Now, as we join these together, we get our second matrix of the LDiA model, the **topic-term matrix**\n",
    "\n",
    "<img src=\"assets/images/03/img_38.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11463617-fc95-45b4-addd-b8ce2c540daf",
   "metadata": {},
   "source": [
    "# <a id='9'>9: Combining the Models</a>\n",
    "\n",
    "Now, if we put it all together and see how to get these two matrices from the LDiA model based on their respective Dirichlet distributions.\n",
    "\n",
    "The rough idea we just saw was the entries fro mthe first matrix matrix come from picking points in the distribution $\\alpha$, the entries in the second matrix come from picking points in the distribution $\\beta$, and the idea is to find the best locations of these points to get the best factorization of the matrix. The best locations of these points will give us *precisely* the topics we want!\n",
    "\n",
    "<img src=\"assets/images/03/img_39.png\" width=700 align='center'>\n",
    "\n",
    "Let's begin by generating some documents to compare them with the originals. We begin with the Dirichlet distribution for topics, $\\alpha$. From here, we draw some points corresponding to all the documents. Each point will give some values for each of the topics which will generate a multivariate distribution $\\theta$ (the mixture of topics corresponding to a document). Now, let's generate some words from document one as follows.\n",
    "\n",
    "From $\\theta$, we draw some topics. How many topics? We will have a Poisson variable, another parameter in the model, to tell us how many! So, we draw some topics based on the probability given by the $\\theta$ distribution. Which, for this example, we'll draw science with a 0.7 probability, politics with 0.2 probability, and sports with 0.1 probability. Now, we'll associate words to these topics using the words Dirichlet distribution $\\beta$.\n",
    "\n",
    "In the dsitribution $\\beta$, we locate the topic, and from each of the dots, we obtain a distribution of the words generated by each of the topics with some probability of each word. These distributions are called $\\phi$ (phi).\n",
    "\n",
    "For each of the topics we've chosen, we'll pick a word associated to it using the multivariate distribution $\\phi$. For eacmple, for the first topic we have science. We look at the science row in the $\\phi$ distribution and pick a word from there. For example, space. So, space is the first word in document one. We now do this for every one of our topics and then generate words from our first generated document, let's call it \"fake document one\". We then do this again, draw another point from $\\alpha$, get another multivariable distribution $\\theta$, which generates new topics from $\\beta$ (let's call that \"fake document two\"). We do this many times, generating many documents. Finally, we compare them with the original documents!\n",
    "\n",
    "To compare the generated documents with the originals, we can use maximum likelihood to figure out the arrangements of points which will give us the real articles with the highest probability.\n",
    "\n",
    "<img src=\"assets/images/03/img_40.png\" width=700 align='center'>\n",
    "\n",
    "In summary, here's what we're doing. We have the two dirichlet distributions, $\\alpha$ and $\\beta$. From $\\alpha$, we pick some documents, and from $\\beta$, we pick some topics. We use these two combined to create some fake articles. Then, we compare the fake articles to the real articles. The probability of obtaining the real articles is, of course, really small, but there must be some arrangement of points in the above distribution that maximizes this probability. Our goal is to find the arrangement of points and that will give us the topics. In the same way that we train many algorithms in machine learning, there will be an error that will tell us how far we are from generating the real articles. The error will back-propogate all the way to the distributions, giving us a gradient that will tell us where to move the points in order to reduce this error.\n",
    "\n",
    "<img src=\"assets/images/03/img_41.png\" width=700 align='center'>\n",
    "\n",
    "So, we move the points as indicated and now we have obtatined a slightly better model. Doing this repeatedly will give us a good enough arrangement of the points. Naturally, a good arrangement of the points will give us some topics.\n",
    "\n",
    "<img src=\"assets/images/03/img_42.png\" width=700 align='center'>\n",
    "\n",
    "The dirichlet distribution $\\alpha$ will tell us what articles are associated to these topics and the dirichlet distribution $\\beta$ will tell us what words are associated to those topics. We can go a bit further and actually back-propogate the error all the way to $\\alpha$ and $\\beta$ obtaining not only better point arrangements but actually better distributions, $\\alpha^{'}$ and $\\beta^{'}$.\n",
    "\n",
    "<img src=\"assets/images/03/img_43.png\" width=700 align='center'>\n",
    "\n",
    "And that's it! That's how latent dirichlate allocation works!\n",
    "\n",
    "\n",
    "If you want to understand the diagram in the paper, here it is:\n",
    "* $\\alpha$ is the topic distribution\n",
    "* $\\beta$ is the word distribution\n",
    "* $\\theta$ is the multivariate distribtion drawn from the topics\n",
    "* $\\phi$ is the multivariate distribution drawn from the words\n",
    "* $z$ is the topics\n",
    "* $w$ is the document obtained by combining the two matrices\n",
    "\n",
    "<img src=\"assets/images/03/img_44.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011c53e1-ac98-47ed-816e-96607285b32b",
   "metadata": {},
   "source": [
    "## <a id='9.1'>9.1: Summary</a>\n",
    "\n",
    "In this lesson, we covered:\n",
    "1. Bag of words in more detail\n",
    "2. Latent variables\n",
    "3. Matrix representation of Latent Dirichlet Allocation\n",
    "4. Beta distribution\n",
    "5. Dirichlet distribution\n",
    "6. More on Latent Dirichlet Allocation\n",
    "7. Sample a topic\n",
    "8. Sample a word\n",
    "9. Combing the models\n",
    "10. Topic modeling lab\n",
    "\n",
    "**Use Cases:**\n",
    "* Topic modeling, document categorization\n",
    "* Mixture of topics in a new document\n",
    "* Generate collections of words with desired mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ecf2f-2077-4a01-9afd-161f97879a71",
   "metadata": {},
   "source": [
    "# <a id='10'>10: Topic Modeling Lab</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc47faa-32cf-4852-83d5-e424e66037f6",
   "metadata": {},
   "source": [
    "## <a id='10.0'>10.0: Step 0 - Intro - Latent Dirichlet Allocation</a>\n",
    "\n",
    "LDiA is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions. \n",
    "\n",
    "* Each document is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words.\n",
    "* LDA assumes that the every chunk of text we feed into it will contain words that are somehow related. Therefore choosing the right corpus of data is crucial. \n",
    "* It also assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922a73e-05cb-4d59-abde-9d299e22517e",
   "metadata": {},
   "source": [
    "## <a id='10.1'>10.1: Step 1 - Load the dataset</a>\n",
    "\n",
    "The dataset we'll use is a list of over one million news headlines published over a period of 15 years. We'll start by loading it from the `02_computing-with-NLP/assets/ldia/abcnews-date-text.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adc61bee-0b22-4416-90f7-bcea8169b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cfebfbb-a12d-4a8b-8df6-847f375e6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_items = ['assets','ldia','abcnews-date-text.csv']\n",
    "file_path = Path(*dir_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e75a2bd-7381-4467-a54e-68072a452a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,103,665 rows by 2 columns\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1103665 entries, 0 to 1103664\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count    Dtype \n",
      "---  ------         --------------    ----- \n",
      " 0   publish_date   1103665 non-null  int64 \n",
      " 1   headline_text  1103665 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 16.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "display(data.head())\n",
    "print(f\"{data.shape[0]:,} rows by {data.shape[1]:,} columns\")\n",
    "print()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a02c8ab9-7a7a-432b-8069-72f6329b9eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using only the first 300k documents; the tutorial includes the column for the index (no need to do this seems relevant just yet)\n",
    "data_text = data[:300000][['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "data_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5830ebe-3dee-40e5-855b-10034e9e1ef5",
   "metadata": {},
   "source": [
    "## <a id='10.2'>10.2: Step 2 - Data Preprocessing</a>\n",
    "\n",
    "We will perform the following steps:\n",
    "\n",
    "* **Tokenization**: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "* Words that have fewer than 3 characters are removed.\n",
    "* All **stopwords** are removed.\n",
    "* Words are **lemmatized** - words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "* Words are **stemmed** - words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5efbd9be-e4a5-453d-9f04-575585005d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/christopherdaigle/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c31c0-9420-4fce-899e-4fcbdf55c0a3",
   "metadata": {},
   "source": [
    "### Lemmatizer Example\n",
    "Before preprocessing our dataset, let's first look at an lemmatizing example. What would be the output if we lemmatized the word 'went':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cee67f5-492e-49cf-ada7-147c40d5e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ee822ff-f35c-4b72-9198-bc8e785fa719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos ='v')) # past tense to present tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b48486e5-d171-48b4-9cde-02d489551a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caresses: caress\n",
      "flies: fly\n",
      "dies: dy\n",
      "mules: mule\n",
      "denied: denied\n",
      "died: died\n",
      "agreed: agreed\n",
      "owned: owned\n",
      "humbled: humbled\n",
      "sized: sized\n",
      "meeting: meeting\n",
      "stating: stating\n",
      "siezing: siezing\n",
      "itemization: itemization\n",
      "sensational: sensational\n",
      "traditional: traditional\n",
      "reference: reference\n",
      "colonizer: colonizer\n",
      "plotted: plotted\n",
      "went: went\n"
     ]
    }
   ],
   "source": [
    "for word in original_words + ['went']:\n",
    "    print(f\"{word}: {WordNetLemmatizer().lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e7b374-115f-4035-a0d1-c2d89421b878",
   "metadata": {},
   "source": [
    "### Stemmer Example\n",
    "Let's also look at a stemming example. Let's throw a number of words at the stemmer and see how it deals with each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "545cc5ad-c2ad-4a08-b4cc-146cc9864822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "\n",
    "pd.DataFrame(data={'original word':original_words, 'stemmed':singles })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d11c0adc-1fcb-4cf5-97f8-1cfa110239e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a025a99-57e4-42f4-8164-ca6b3a94e092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['rain', 'helps', 'dampen', 'bushfires']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['rain', 'help', 'dampen', 'bushfir']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview a document after preprocessing\n",
    "'''\n",
    "document_num = 4310\n",
    "doc_sample = documents[documents['index'] == document_num].values[0][0]\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c6b61-1cde-45a8-a0f6-c7b20472c070",
   "metadata": {},
   "source": [
    "Let's now preprocess all the news headlines we have. To do that, let's use the [map](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html) function from pandas to apply `preprocess()` to the `headline_text` column\n",
    "\n",
    "**Note**: This may take a few minutes (this took the instructor 6 minutes on his laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b47d72e7-f288-4c4e-a439-e7f3f279f16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c1a4e24-f1db-4741-bb2c-ea6eb9a73221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 s, sys: 82.9 ms, total: 20.8 s\n",
      "Wall time: 20.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "      <th>proc_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[decid, communiti, broadcast, licenc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "      <td>[wit, awar, defam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "      <td>[call, infrastructur, protect, summit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "      <td>[staff, aust, strike, rise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "      <td>[strike, affect, australian, travel]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index  \\\n",
       "0  aba decides against community broadcasting lic...      0   \n",
       "1     act fire witnesses must be aware of defamation      1   \n",
       "2     a g calls for infrastructure protection summit      2   \n",
       "3           air nz staff in aust strike for pay rise      3   \n",
       "4      air nz strike to affect australian travellers      4   \n",
       "\n",
       "                                 proc_doc  \n",
       "0   [decid, communiti, broadcast, licenc]  \n",
       "1                      [wit, awar, defam]  \n",
       "2  [call, infrastructur, protect, summit]  \n",
       "3             [staff, aust, strike, rise]  \n",
       "4    [strike, affect, australian, travel]  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# TODO: preprocess all the headlines, saving the list of results as 'processed_docs'\n",
    "documents['proc_doc'] = documents['headline_text'].map(preprocess)\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af288641-5393-4c57-9083-eaeef31e1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['proc_doc'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1807618-fd08-4d97-8c3a-e6e620885e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['decid', 'communiti', 'broadcast', 'licenc'],\n",
       " ['wit', 'awar', 'defam'],\n",
       " ['call', 'infrastructur', 'protect', 'summit'],\n",
       " ['staff', 'aust', 'strike', 'rise'],\n",
       " ['strike', 'affect', 'australian', 'travel'],\n",
       " ['ambiti', 'olsson', 'win', 'tripl', 'jump'],\n",
       " ['antic', 'delight', 'record', 'break', 'barca'],\n",
       " ['aussi', 'qualifi', 'stosur', 'wast', 'memphi', 'match'],\n",
       " ['aust', 'address', 'secur', 'council', 'iraq'],\n",
       " ['australia', 'lock', 'timet']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Preview 'processed_docs'\n",
    "'''\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594c749-1774-4f2e-a1f9-0f416aab047a",
   "metadata": {},
   "source": [
    "## <a id='10.3'>10.3: Step 3.1 - Bag of words on the dataset</a>\n",
    "\n",
    "Now let's create a dictionary from 'processed_docs' containing the number of times a word appears in the training set. To do that, let's pass `processed_docs` to [`gensim.corpora.Dictionary()`](https://radimrehurek.com/gensim/corpora/dictionary.html) and call it '`dictionary`'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5277cc7d-e6ec-4628-b87f-de9f225f6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e403f7a-70b0-4797-bf89-cfd76604787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6517200-0260-4b71-b7a8-1989cfbb8a58",
   "metadata": {},
   "source": [
    "** Gensim filter_extremes **\n",
    "\n",
    "[`filter_extremes(no_below=5, no_above=0.5, keep_n=100000)`](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes)\n",
    "\n",
    "Filter out tokens that appear in\n",
    "\n",
    "* less than no_below documents (absolute number) or\n",
    "* more than no_above documents (fraction of total corpus size, not absolute number).\n",
    "* after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f9d542f1-82f3-4d27-8a41-0af52d1fde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "# TODO: apply dictionary.filter_extremes() with the parameters mentioned above\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b8a640-cadc-40ad-a7df-ce5d9c475674",
   "metadata": {},
   "source": [
    "** Gensim doc2bow **\n",
    "\n",
    "[`doc2bow(document)`](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2bow)\n",
    "\n",
    "* Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce56db93-a2ac-4a74-857d-dca343054d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "# TODO\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9fe969e0-5c57-4083-bd95-ab85f0e475e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(71, 1), (107, 1), (462, 1), (3530, 1)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Checking Bag of Words corpus for our sample document --> (token_id, token_count)\n",
    "'''\n",
    "bow_corpus[document_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d4f5c99d-f211-45da-9fef-ea46386b45c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 71 (\"bushfir\") appears 1 time.\n",
      "Word 107 (\"help\") appears 1 time.\n",
      "Word 462 (\"rain\") appears 1 time.\n",
      "Word 3530 (\"dampen\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "# Here document_num is document number 4310 which we have checked in Step 2\n",
    "bow_doc_4310 = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1e42b-ba12-48a3-852f-3bd1a34c088c",
   "metadata": {},
   "source": [
    "## <a id='10.4'>10.4: Step 3.2 - TF-IDF on our document set</a>\n",
    "\n",
    "While performing TF-IDF on the corpus is not necessary for LDA implemention using the gensim model, it is recemmended. TF-IDF expects a bag-of-words (integer values) training corpus during initialization. During transformation, it will take a vector and return another vector of the same dimensionality.\n",
    "\n",
    "*Please note: The author of Gensim dictates the standard procedure for LDA to be using the Bag of Words model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816a45d-c335-4490-b5fe-5edcd348f00e",
   "metadata": {},
   "source": [
    "** TF-IDF stands for \"Term Frequency, Inverse Document Frequency\".**\n",
    "\n",
    "* It is a way to score the importance of words (or \"terms\") in a document based on how frequently they appear across multiple documents.\n",
    "* If a word appears frequently in a document, it's important. Give the word a high score. But if a word appears in many documents, it's not a unique identifier. Give the word a low score.\n",
    "* Therefore, common words like \"the\" and \"for\", which appear in many documents, will be scaled down. Words that appear frequently in a single document will be scaled up.\n",
    "\n",
    "In other words:\n",
    "\n",
    "* TF(w) = `(Number of times term w appears in a document) / (Total number of terms in the document)`.\n",
    "* IDF(w) = `log_e(Total number of documents / Number of documents with term w in it)`.\n",
    "\n",
    "** For example **\n",
    "\n",
    "* Consider a document containing `100` words wherein the word 'tiger' appears 3 times. \n",
    "* The term frequency (i.e., tf) for 'tiger' is then: \n",
    "    - `TF = (3 / 100) = 0.03`. \n",
    "\n",
    "* Now, assume we have `10 million` documents and the word 'tiger' appears in `1000` of these. Then, the inverse document frequency (i.e., idf) is calculated as:\n",
    "    - `IDF = log(10,000,000 / 1,000) = 4`. \n",
    "\n",
    "* Thus, the Tf-idf weight is the product of these quantities: \n",
    "    - `TF-IDF = 0.03 * 4 = 0.12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6540b01a-a8f8-43ed-b2d5-8efad15b9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create tf-idf model object using models.TfidfModel on 'bow_corpus' and save it to 'tfidf'\n",
    "'''\n",
    "from gensim import corpora, models\n",
    "\n",
    "# TODO\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# >>> corpus = [dct.doc2bow(line) for line in dataset]  # convert corpus to BoW format\n",
    "# >>>\n",
    "# >>> model = TfidfModel(corpus)  # fit model\n",
    "# >>> vector = model[corpus[0]]  # apply model to the first corpus document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20447b19-701e-42db-aabf-2cf124aa8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Apply transformation to the entire corpus and call it 'corpus_tfidf'\n",
    "'''\n",
    "# TODO\n",
    "corpus_tfidf = [tfidf[doc] for doc in bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "77330e82-e593-446d-9a9c-28efb777221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5959813347777092),\n",
      " (1, 0.39204529549491984),\n",
      " (2, 0.48531419274988147),\n",
      " (3, 0.5055461098578569)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview TF-IDF scores for our first document --> --> (token_id, tfidf score)\n",
    "'''\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755484a-33c8-423f-a23d-85a617add155",
   "metadata": {},
   "source": [
    "## <a id='10.5'>Step 4.1: Running LDA using Bag of Words</a>\n",
    "\n",
    "We are going for 10 topics in the document corpus.\n",
    "\n",
    "** We will be running LDiA using all CPU cores to parallelize and speed up model training.**\n",
    "\n",
    "Some of the parameters we will be tweaking are:\n",
    "\n",
    "* **num_topics** is the number of requested latent topics to be extracted from the training corpus.\n",
    "* **id2word** is a mapping from word ids (integers) to words (strings). It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "* **workers** is the number of extra processes to use for parallelization. Uses all available cores by default.\n",
    "* **alpha** and **eta** are hyperparameters that affect sparsity of the document-topic (theta) and topic-word (lambda) distributions. We will let these be the default values for now(default value is `1/num_topics`)\n",
    "    - Alpha is the per document topic distribution.\n",
    "        * High alpha: Every document has a mixture of all topics(documents appear similar to each other).\n",
    "        * Low alpha: Every document has a mixture of very few topics\n",
    "\n",
    "    - Eta is the per topic word distribution.\n",
    "        * High eta: Each topic has a mixture of most words(topics appear similar to each other).\n",
    "        * Low eta: Each topic has a mixture of few words.\n",
    "\n",
    "* ** passes ** is the number of training passes through the corpus. For  example, if the training corpus has 50,000 documents, chunksize is  10,000, passes is 2, then online training is done in 10 updates: \n",
    "    * `#1 documents 0-9,999 `\n",
    "    * `#2 documents 10,000-19,999 `\n",
    "    * `#3 documents 20,000-29,999 `\n",
    "    * `#4 documents 30,000-39,999 `\n",
    "    * `#5 documents 40,000-49,999 `\n",
    "    * `#6 documents 0-9,999 `\n",
    "    * `#7 documents 10,000-19,999 `\n",
    "    * `#8 documents 20,000-29,999 `\n",
    "    * `#9 documents 30,000-39,999 `\n",
    "    * `#10 documents 40,000-49,999` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d63b96d2-a739-4496-893a-a0a51ad99c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.35 s, sys: 1.02 s, total: 7.37 s\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, \n",
    "#                                    num_topics = 10, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 50)\n",
    "\n",
    "# LDA multicore \n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "# TODO\n",
    "lda_model = gensim.models.LdaMulticore(\n",
    "    corpus=bow_corpus, # input data for model\n",
    "    num_topics=10, # number of topics for the model to identify\n",
    "    id2word=dictionary, # Mapping from word IDs to words - from the filtered dictionary\n",
    "    passes=3, # number of times for the model to go over the data\n",
    "    workers=31 # number of actual cores of a CPU to train over (I have 32, leaving one open)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d1206d6f-ec8a-43c2-b8ae-e4a69be5f006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.019*\"polic\" + 0.014*\"crash\" + 0.009*\"murder\" + 0.008*\"coast\" + 0.008*\"die\" + 0.007*\"iraq\" + 0.007*\"woman\" + 0.007*\"claim\" + 0.006*\"investig\" + 0.006*\"boost\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.010*\"lead\" + 0.009*\"govt\" + 0.007*\"death\" + 0.007*\"polic\" + 0.007*\"jail\" + 0.006*\"chang\" + 0.006*\"charg\" + 0.005*\"claim\" + 0.005*\"court\" + 0.005*\"doubt\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.021*\"plan\" + 0.010*\"govt\" + 0.009*\"polic\" + 0.008*\"back\" + 0.008*\"council\" + 0.007*\"protest\" + 0.007*\"group\" + 0.007*\"closer\" + 0.007*\"fund\" + 0.006*\"support\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.015*\"polic\" + 0.009*\"mayor\" + 0.009*\"farmer\" + 0.008*\"urg\" + 0.008*\"drought\" + 0.008*\"test\" + 0.008*\"group\" + 0.007*\"govt\" + 0.007*\"target\" + 0.007*\"consid\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.015*\"kill\" + 0.010*\"attack\" + 0.010*\"plan\" + 0.008*\"council\" + 0.007*\"hold\" + 0.007*\"report\" + 0.007*\"say\" + 0.007*\"offer\" + 0.006*\"iraq\" + 0.006*\"develop\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.014*\"council\" + 0.011*\"water\" + 0.010*\"hospit\" + 0.008*\"plan\" + 0.008*\"claim\" + 0.008*\"govt\" + 0.008*\"centr\" + 0.006*\"face\" + 0.006*\"decis\" + 0.005*\"want\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.011*\"hous\" + 0.010*\"warn\" + 0.010*\"govt\" + 0.009*\"power\" + 0.008*\"servic\" + 0.007*\"public\" + 0.006*\"record\" + 0.006*\"return\" + 0.005*\"polic\" + 0.005*\"emerg\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.022*\"govt\" + 0.015*\"court\" + 0.015*\"charg\" + 0.015*\"urg\" + 0.013*\"face\" + 0.008*\"opposit\" + 0.008*\"dead\" + 0.008*\"boost\" + 0.007*\"water\" + 0.007*\"council\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.010*\"polic\" + 0.010*\"say\" + 0.010*\"union\" + 0.008*\"worker\" + 0.007*\"death\" + 0.007*\"strike\" + 0.006*\"plan\" + 0.006*\"health\" + 0.006*\"protest\" + 0.005*\"defend\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.013*\"polic\" + 0.010*\"bomb\" + 0.008*\"say\" + 0.008*\"kill\" + 0.007*\"labor\" + 0.006*\"troop\" + 0.006*\"iraq\" + 0.006*\"talk\" + 0.006*\"india\" + 0.005*\"pakistan\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6c36c-58a6-4729-b721-b656d4149bdb",
   "metadata": {},
   "source": [
    "### Classification of the topics ###\n",
    "\n",
    "Using the words in each topic and their corresponding weights, what categories were you able to infer?\n",
    "\n",
    "* 0: 'political labor'\n",
    "* 1: 'catastrophe'\n",
    "* 2: 'more catastrophe'\n",
    "* 3: \n",
    "* 4: \n",
    "* 5: \n",
    "* 6: \n",
    "* 7:  \n",
    "* 8: \n",
    "* 9:\n",
    "\n",
    "**These are hard to distinguish**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f7619-3f1f-42bc-86c6-77bb688045a1",
   "metadata": {},
   "source": [
    "## <a id='10.6'>10.6: Step 4.2 - Running LDA using TF-IDF</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6f580690-ab47-42f1-964f-8f1f48114377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 s, sys: 1.32 s, total: 13.3 s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Define lda model using corpus_tfidf\n",
    "'''\n",
    "# TODO\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(\n",
    "    corpus=corpus_tfidf, # input data for model\n",
    "    num_topics=10, # number of topics for the model to identify\n",
    "    id2word=dictionary, # Mapping from word IDs to words - from the filtered dictionary\n",
    "    passes=3, # number of times for the model to go over the data\n",
    "    workers=31 # number of actual cores of a CPU to train over (I have 32, leaving one open)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4848d9d0-7a7a-4c7a-b98d-ce415d5fc0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.006*\"polic\" + 0.004*\"stab\" + 0.004*\"boost\" + 0.004*\"injur\" + 0.003*\"govt\" + 0.003*\"damag\" + 0.003*\"fund\" + 0.003*\"plan\" + 0.003*\"murder\" + 0.003*\"sydney\"\n",
      "\n",
      "\n",
      "Topic: 1 Word: 0.018*\"polic\" + 0.008*\"crash\" + 0.008*\"iraq\" + 0.007*\"investig\" + 0.006*\"miss\" + 0.006*\"die\" + 0.006*\"council\" + 0.006*\"driver\" + 0.005*\"search\" + 0.005*\"fatal\"\n",
      "\n",
      "\n",
      "Topic: 2 Word: 0.006*\"guilti\" + 0.005*\"charg\" + 0.004*\"plead\" + 0.004*\"court\" + 0.004*\"say\" + 0.004*\"titl\" + 0.004*\"world\" + 0.004*\"telstra\" + 0.003*\"murder\" + 0.003*\"face\"\n",
      "\n",
      "\n",
      "Topic: 3 Word: 0.008*\"charg\" + 0.006*\"govt\" + 0.006*\"face\" + 0.005*\"plan\" + 0.005*\"assault\" + 0.005*\"court\" + 0.004*\"seek\" + 0.004*\"polic\" + 0.004*\"victim\" + 0.004*\"child\"\n",
      "\n",
      "\n",
      "Topic: 4 Word: 0.008*\"water\" + 0.005*\"plan\" + 0.004*\"council\" + 0.004*\"polic\" + 0.004*\"govt\" + 0.003*\"terror\" + 0.003*\"kill\" + 0.003*\"continu\" + 0.003*\"reject\" + 0.003*\"futur\"\n",
      "\n",
      "\n",
      "Topic: 5 Word: 0.005*\"govt\" + 0.005*\"plan\" + 0.005*\"blaze\" + 0.004*\"rescu\" + 0.004*\"polic\" + 0.004*\"rail\" + 0.003*\"kill\" + 0.003*\"health\" + 0.003*\"blast\" + 0.003*\"highway\"\n",
      "\n",
      "\n",
      "Topic: 6 Word: 0.013*\"closer\" + 0.006*\"govt\" + 0.005*\"plan\" + 0.005*\"court\" + 0.004*\"urg\" + 0.004*\"teacher\" + 0.004*\"drought\" + 0.004*\"power\" + 0.003*\"wast\" + 0.003*\"dump\"\n",
      "\n",
      "\n",
      "Topic: 7 Word: 0.005*\"govt\" + 0.005*\"market\" + 0.005*\"industri\" + 0.005*\"council\" + 0.005*\"boost\" + 0.004*\"plan\" + 0.004*\"record\" + 0.003*\"fund\" + 0.003*\"offer\" + 0.003*\"staff\"\n",
      "\n",
      "\n",
      "Topic: 8 Word: 0.007*\"open\" + 0.007*\"lead\" + 0.005*\"england\" + 0.004*\"govt\" + 0.004*\"crash\" + 0.004*\"home\" + 0.004*\"kill\" + 0.004*\"polic\" + 0.004*\"australia\" + 0.003*\"bomb\"\n",
      "\n",
      "\n",
      "Topic: 9 Word: 0.005*\"govt\" + 0.005*\"say\" + 0.005*\"elect\" + 0.004*\"cost\" + 0.004*\"plan\" + 0.004*\"council\" + 0.004*\"urg\" + 0.004*\"kill\" + 0.004*\"face\" + 0.004*\"claim\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1df6d5-d4b6-4486-b009-1edbbe845d8c",
   "metadata": {},
   "source": [
    "### Classification of the topics ###\n",
    "\n",
    "As we can see, when using tf-idf, heavier weights are given to words that are not as frequent which results in nouns being factored in. That makes it harder to figure out the categories as nouns can be hard to categorize. This goes to show that the models we apply depend on the type of corpus of text we are dealing with. \n",
    "\n",
    "Using the words in each topic and their corresponding weights, what categories could you find? **skipping this step**\n",
    "\n",
    "* 0: \n",
    "* 1:  \n",
    "* 2: \n",
    "* 3: \n",
    "* 4:  \n",
    "* 5: \n",
    "* 6: \n",
    "* 7: \n",
    "* 8: \n",
    "* 9: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b3b78-b25b-4c04-8cd8-8fdc8d9b6ab2",
   "metadata": {},
   "source": [
    "## <a id='10.7'>10.7: Step 5.1 - Performance evaluation by classifying sample document using LDA Bag of Words model</a>\n",
    "\n",
    "We will check to see where our test document would be classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "272858d5-ce4a-461f-a665-fa9d756b8dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rain', 'help', 'dampen', 'bushfir']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Text of sample document 4310\n",
    "'''\n",
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0888e04b-7af8-4bfc-8175-e2379f2e9469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8199149966239929\t \n",
      "Topic: 0.015*\"polic\" + 0.009*\"mayor\" + 0.009*\"farmer\" + 0.008*\"urg\" + 0.008*\"drought\" + 0.008*\"test\" + 0.008*\"group\" + 0.007*\"govt\" + 0.007*\"target\" + 0.007*\"consid\"\n",
      "\n",
      "Score: 0.02001369558274746\t \n",
      "Topic: 0.011*\"hous\" + 0.010*\"warn\" + 0.010*\"govt\" + 0.009*\"power\" + 0.008*\"servic\" + 0.007*\"public\" + 0.006*\"record\" + 0.006*\"return\" + 0.005*\"polic\" + 0.005*\"emerg\"\n",
      "\n",
      "Score: 0.020012564957141876\t \n",
      "Topic: 0.015*\"kill\" + 0.010*\"attack\" + 0.010*\"plan\" + 0.008*\"council\" + 0.007*\"hold\" + 0.007*\"report\" + 0.007*\"say\" + 0.007*\"offer\" + 0.006*\"iraq\" + 0.006*\"develop\"\n",
      "\n",
      "Score: 0.020010806620121002\t \n",
      "Topic: 0.019*\"polic\" + 0.014*\"crash\" + 0.009*\"murder\" + 0.008*\"coast\" + 0.008*\"die\" + 0.007*\"iraq\" + 0.007*\"woman\" + 0.007*\"claim\" + 0.006*\"investig\" + 0.006*\"boost\"\n",
      "\n",
      "Score: 0.02000867947936058\t \n",
      "Topic: 0.022*\"govt\" + 0.015*\"court\" + 0.015*\"charg\" + 0.015*\"urg\" + 0.013*\"face\" + 0.008*\"opposit\" + 0.008*\"dead\" + 0.008*\"boost\" + 0.007*\"water\" + 0.007*\"council\"\n",
      "\n",
      "Score: 0.020008308812975883\t \n",
      "Topic: 0.013*\"polic\" + 0.010*\"bomb\" + 0.008*\"say\" + 0.008*\"kill\" + 0.007*\"labor\" + 0.006*\"troop\" + 0.006*\"iraq\" + 0.006*\"talk\" + 0.006*\"india\" + 0.005*\"pakistan\"\n",
      "\n",
      "Score: 0.020008105784654617\t \n",
      "Topic: 0.010*\"lead\" + 0.009*\"govt\" + 0.007*\"death\" + 0.007*\"polic\" + 0.007*\"jail\" + 0.006*\"chang\" + 0.006*\"charg\" + 0.005*\"claim\" + 0.005*\"court\" + 0.005*\"doubt\"\n",
      "\n",
      "Score: 0.020008068531751633\t \n",
      "Topic: 0.014*\"council\" + 0.011*\"water\" + 0.010*\"hospit\" + 0.008*\"plan\" + 0.008*\"claim\" + 0.008*\"govt\" + 0.008*\"centr\" + 0.006*\"face\" + 0.006*\"decis\" + 0.005*\"want\"\n",
      "\n",
      "Score: 0.02000788412988186\t \n",
      "Topic: 0.021*\"plan\" + 0.010*\"govt\" + 0.009*\"polic\" + 0.008*\"back\" + 0.008*\"council\" + 0.007*\"protest\" + 0.007*\"group\" + 0.007*\"closer\" + 0.007*\"fund\" + 0.006*\"support\"\n",
      "\n",
      "Score: 0.020006902515888214\t \n",
      "Topic: 0.010*\"polic\" + 0.010*\"say\" + 0.010*\"union\" + 0.008*\"worker\" + 0.007*\"death\" + 0.007*\"strike\" + 0.006*\"plan\" + 0.006*\"health\" + 0.006*\"protest\" + 0.005*\"defend\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check which topic our test document belongs to using the LDA Bag of Words model.\n",
    "'''\n",
    "\n",
    "# Our test document is document number 4310\n",
    "for index, score in sorted(lda_model[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013db48a-f671-4650-91d4-9663d020bdcf",
   "metadata": {},
   "source": [
    "### It has the highest probability (`82%`) to be  part of the topic that we assigned as X, which is the accurate classification. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f147d5e-1470-4373-9452-cd393aa15fd3",
   "metadata": {},
   "source": [
    "## <a id='10.8'>10.8: Step 5.2 - Performance evaluation by classifying sample document using LDA TF-IDF model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3865ea6-e9a3-4185-9615-ecea0caab88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.44177496433258057\t \n",
      "Topic: 0.005*\"govt\" + 0.005*\"say\" + 0.005*\"elect\" + 0.004*\"cost\" + 0.004*\"plan\" + 0.004*\"council\" + 0.004*\"urg\" + 0.004*\"kill\" + 0.004*\"face\" + 0.004*\"claim\"\n",
      "\n",
      "Score: 0.39813169836997986\t \n",
      "Topic: 0.005*\"govt\" + 0.005*\"market\" + 0.005*\"industri\" + 0.005*\"council\" + 0.005*\"boost\" + 0.004*\"plan\" + 0.004*\"record\" + 0.003*\"fund\" + 0.003*\"offer\" + 0.003*\"staff\"\n",
      "\n",
      "Score: 0.020013555884361267\t \n",
      "Topic: 0.008*\"charg\" + 0.006*\"govt\" + 0.006*\"face\" + 0.005*\"plan\" + 0.005*\"assault\" + 0.005*\"court\" + 0.004*\"seek\" + 0.004*\"polic\" + 0.004*\"victim\" + 0.004*\"child\"\n",
      "\n",
      "Score: 0.020012762397527695\t \n",
      "Topic: 0.008*\"water\" + 0.005*\"plan\" + 0.004*\"council\" + 0.004*\"polic\" + 0.004*\"govt\" + 0.003*\"terror\" + 0.003*\"kill\" + 0.003*\"continu\" + 0.003*\"reject\" + 0.003*\"futur\"\n",
      "\n",
      "Score: 0.02001274563372135\t \n",
      "Topic: 0.005*\"govt\" + 0.005*\"plan\" + 0.005*\"blaze\" + 0.004*\"rescu\" + 0.004*\"polic\" + 0.004*\"rail\" + 0.003*\"kill\" + 0.003*\"health\" + 0.003*\"blast\" + 0.003*\"highway\"\n",
      "\n",
      "Score: 0.02001209557056427\t \n",
      "Topic: 0.018*\"polic\" + 0.008*\"crash\" + 0.008*\"iraq\" + 0.007*\"investig\" + 0.006*\"miss\" + 0.006*\"die\" + 0.006*\"council\" + 0.006*\"driver\" + 0.005*\"search\" + 0.005*\"fatal\"\n",
      "\n",
      "Score: 0.020011356100440025\t \n",
      "Topic: 0.007*\"open\" + 0.007*\"lead\" + 0.005*\"england\" + 0.004*\"govt\" + 0.004*\"crash\" + 0.004*\"home\" + 0.004*\"kill\" + 0.004*\"polic\" + 0.004*\"australia\" + 0.003*\"bomb\"\n",
      "\n",
      "Score: 0.02001107856631279\t \n",
      "Topic: 0.013*\"closer\" + 0.006*\"govt\" + 0.005*\"plan\" + 0.005*\"court\" + 0.004*\"urg\" + 0.004*\"teacher\" + 0.004*\"drought\" + 0.004*\"power\" + 0.003*\"wast\" + 0.003*\"dump\"\n",
      "\n",
      "Score: 0.0200104471296072\t \n",
      "Topic: 0.006*\"polic\" + 0.004*\"stab\" + 0.004*\"boost\" + 0.004*\"injur\" + 0.003*\"govt\" + 0.003*\"damag\" + 0.003*\"fund\" + 0.003*\"plan\" + 0.003*\"murder\" + 0.003*\"sydney\"\n",
      "\n",
      "Score: 0.020009255036711693\t \n",
      "Topic: 0.006*\"guilti\" + 0.005*\"charg\" + 0.004*\"plead\" + 0.004*\"court\" + 0.004*\"say\" + 0.004*\"titl\" + 0.004*\"world\" + 0.004*\"telstra\" + 0.003*\"murder\" + 0.003*\"face\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check which topic our test document belongs to using the LDA TF-IDF model.\n",
    "'''\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada259f-da6b-4e8b-b236-8787a863cab7",
   "metadata": {},
   "source": [
    "### It has the highest probability (`44%`) to be  part of the topic that we assigned as X. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fdca4-2720-4d6f-ac36-f8f661777f96",
   "metadata": {},
   "source": [
    "## <a id='10.9'>10.9: Step 6 - Testing model on unseen document</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d9266b5-e653-4934-a530-b21a4af8497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8198835849761963\t Topic: 0.010*\"lead\" + 0.009*\"govt\" + 0.007*\"death\" + 0.007*\"polic\" + 0.007*\"jail\"\n",
      "Score: 0.0200209803879261\t Topic: 0.011*\"hous\" + 0.010*\"warn\" + 0.010*\"govt\" + 0.009*\"power\" + 0.008*\"servic\"\n",
      "Score: 0.02001331001520157\t Topic: 0.013*\"polic\" + 0.010*\"bomb\" + 0.008*\"say\" + 0.008*\"kill\" + 0.007*\"labor\"\n",
      "Score: 0.02001316472887993\t Topic: 0.015*\"kill\" + 0.010*\"attack\" + 0.010*\"plan\" + 0.008*\"council\" + 0.007*\"hold\"\n",
      "Score: 0.02001257613301277\t Topic: 0.015*\"polic\" + 0.009*\"mayor\" + 0.009*\"farmer\" + 0.008*\"urg\" + 0.008*\"drought\"\n",
      "Score: 0.020012354478240013\t Topic: 0.022*\"govt\" + 0.015*\"court\" + 0.015*\"charg\" + 0.015*\"urg\" + 0.013*\"face\"\n",
      "Score: 0.02001224085688591\t Topic: 0.019*\"polic\" + 0.014*\"crash\" + 0.009*\"murder\" + 0.008*\"coast\" + 0.008*\"die\"\n",
      "Score: 0.02001161314547062\t Topic: 0.014*\"council\" + 0.011*\"water\" + 0.010*\"hospit\" + 0.008*\"plan\" + 0.008*\"claim\"\n",
      "Score: 0.020010674372315407\t Topic: 0.010*\"polic\" + 0.010*\"say\" + 0.010*\"union\" + 0.008*\"worker\" + 0.007*\"death\"\n",
      "Score: 0.020009513944387436\t Topic: 0.021*\"plan\" + 0.010*\"govt\" + 0.009*\"polic\" + 0.008*\"back\" + 0.008*\"council\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"My favorite sports activities are running and swimming.\"\n",
    "\n",
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5db501-e660-4a4b-9a33-62a9d5d7db41",
   "metadata": {},
   "source": [
    "The model correctly classifies the unseen document with '82'% probability to the X category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b401e-dbf2-4d74-b7c7-a525e34c30db",
   "metadata": {},
   "source": [
    "This tutorial is [also available on the Udacity GitHub](https://github.com/udacity/cd0379-computing-with-natural-language/tree/main/2.2-topic-modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789950d9-0d0f-48ef-b194-6dea8b670790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_nlp",
   "language": "python",
   "name": "venv_ud_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
