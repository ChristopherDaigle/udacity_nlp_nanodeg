{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1330dce2-8a38-4cd1-a49a-5268d9a84289",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "[1. Bag-of-Words in More Detail](#1)<br>\n",
    "[2. Latent Variables](#2)<br>\n",
    "[3. Matrix Representation of Latent Dirichlet Allocation](#3)<br>\n",
    "[4. Beta Distribution](#4)<br>\n",
    "[5. Dirichlet Distribution](#5)<br>\n",
    "[6. More on Latent Dirichlet Allocation](#6)<br>\n",
    "[7. Sample a Topic](#7)<br>\n",
    "[8. Sample a Word](#8)<br>\n",
    "[9. Combing the Models](#9)<br>\n",
    "[10. Topic Modeling Lab](#10)<br>\n",
    "\n",
    "## References\n",
    "In this section, we'll be following this article by David Blei, Andrew Ng, and Michael Jordan.\n",
    "* [Latent Dirichlet Allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485017ec-d60f-4687-a9d3-4f64317e4375",
   "metadata": {},
   "source": [
    "# <a id='1'>1: Bag-of-Words (BoW) in More Detail</a>\n",
    "\n",
    "If you think about the BoW model graphically, it represents the relationship between a set of document objects and a set of word objects.\n",
    "\n",
    "Assume we have the article, *Space exploration. A vote to explore space has been explored*, and that we have done a good job processing the text (case, stemming, lemmatization, etc.)\n",
    "* There are three main terms: **space**, **vote**, and **explore**\n",
    "* To find the probability of each term appearing in the article, we divide the count of each term by the total number of terms\n",
    "* We have three parameters - probabilities for each term ( $p(\\text{space|article})$, $p(\\text{vote|article})$, $p(\\text{explore|article})$ )\n",
    "\n",
    "To add some notation:\n",
    "* d: documents (units of groups of terms to be analyzed)\n",
    "* t: terms (elements that compose documents)\n",
    "* P(t|d): probability of a term appearing in the document (\"For any given document, $d$, and observed term, $t$, how likely is it that the document $d$ generated the term $t$\")\n",
    "\n",
    "<img src=\"assets/images/03/img_01.png\" width=700 align='center'>\n",
    "\n",
    "Now, if we do this for many documents, say 500, and many terms, say 1,000, we can get something of the sort:\n",
    "\n",
    "<img src=\"assets/images/03/img_02.png\" width=700 align='center'>\n",
    "\n",
    "If we have 500,000 parameters, that is a lot of parameters to figure out. We can reduce the number of parameters and still keep most of the information by representing the terms in a latent space. This is commonly known as **topic modeling**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6962a2-ca7e-46fc-a84c-1320a233c2e7",
   "metadata": {},
   "source": [
    "# <a id='2'>2: Latent Variables</a>\n",
    "\n",
    "Consider adding to the model the notion of a small set of topics or latent variables (or themes) that actually drive the generation of words in each document. So in this model, any document is considered to have an underlying mixture of topics associated with it. Similarly, a topic is considered to be a mixture of terms that it is likely to generate.\n",
    "\n",
    "If we take our **documents**, our **terms**, and assert there are a number of **topics**, say 3, then we have 2-sets of probability distributions:\n",
    "1. $p(\\text{z|d})$: topic-document probability (probability of topic $z$ given a document $d$)\n",
    "2. $p(\\text{t|z})$: term-topic probability (probability of a term $t$ given a topic $z$)\n",
    "\n",
    "Our new probability of a document given a term, $p(\\text{t|d})$, can be expressed as a sum over the two previous probabilities:\n",
    "$$P\\left( t \\mid d \\right) = \\Sigma_{z} P\\left( t \\mid z \\right) \\cdot P\\left( z \\mid d \\right)$$\n",
    "\n",
    "<img src=\"assets/images/03/img_03.png\" width=700 align='center'>\n",
    "\n",
    "Now, the number of parameters is: (number of documents * number of topics) + (number of topics * number of terms)\n",
    "> * 500 documents, 10 topics, 1,000 words: (500 * 10) + (10 * 1,000) = 15,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61d069e-4a00-4f65-b67e-f47ef4355eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_nlp",
   "language": "python",
   "name": "venv_ud_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
