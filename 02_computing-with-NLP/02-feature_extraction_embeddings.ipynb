{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d20f04-bc06-4164-a030-22151e4ea568",
   "metadata": {},
   "source": [
    "# Feature Extraction and Embeddings\n",
    "\n",
    "[1. Bag of words](#1)<br>\n",
    "[2. Term Frequency Inverse Document Frequency (TF-IDF)](#2)<br>\n",
    "[3. One-hot encoding (OHE)](#3)<br>\n",
    "[4. Word embedding](#4)<br>\n",
    "[5. Word2Vec](#5)<br>\n",
    "[6. GloVe](#6)<br>\n",
    "[7. Embeddings for deep learning](#7)<br>\n",
    "[8. t-SNE](#8)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d99ff6-6895-4206-ab01-011879818868",
   "metadata": {},
   "source": [
    "Once we have our text cleaned and transformed, we need to transform it into features that can be used for modeling. In this section, we will cover methods for doing just that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67cdeb-2113-4666-a51b-6fa78872b202",
   "metadata": {},
   "source": [
    "## <a id='1'>1: Bag-of-Words (Bow)</a>\n",
    "\n",
    "The BoW model treats each document as an un-ordered collection or bag-of-words.\n",
    "\n",
    "A **document** is the *unit of text* you want to analyze. As an example, if you wanted to compare essays submitted by students for plagiarism, each essay would be a **document**.\n",
    "\n",
    "To obtain a BoW from a piece of raw text, you need to simply apply appropriate text processing (cleaning, normalizing, splitting into words, stemming, etc.) and then treat the resulting **tokens** as an *un-ordered* collection or set:\n",
    "\n",
    "$$\\text{Little House on the Prairie} \\rightarrow \\text{{\"littl\", \"hous\", \"prairi\"}}$$\n",
    "$$\\text{Mary had a Little Lamb} \\rightarrow \\text{{\"mari\", \"littl\", \"lamb\"}}$$\n",
    "$$\\text{The Silence of the Lambs} \\rightarrow \\text{{\"silenc\", \"lamb\"}}$$\n",
    "$$\\text{Twinkle Twinkle Little Star} \\rightarrow \\text{{\"twinkl\", \"littl\", \"star\"}}$$\n",
    "\n",
    "But keeping these as separate sets is very inefficient. They are of different sizes, may contain different words, and are hard to compare. What if a word occurs multiple times in a document? A more useful approach is turning each document into a vector of numbers representing how many times each word occurs in a document.\n",
    "\n",
    "A *set of documents* is known as a **corpus**. A corpus gives the context for the vectors to be calculated.\n",
    "\n",
    "**First:** collect all the unique words present in your corpus to form your vocabulary\n",
    "\n",
    "<img src=\"assets/images/img_01.png\" width=700 align='center'>\n",
    "\n",
    "**Second:** arrange these words in some order so that they form the vector element positions, or columns of a table, and assume each document is a row\n",
    "\n",
    "<img src=\"assets/images/img_02.png\" width=700 align='center'>\n",
    "\n",
    "**Third:** count the number of occurences of each word in each document and enter the value in the respective column, to create a **document-term matrix (DTM)**\n",
    "\n",
    "<img src=\"assets/images/img_03.png\" width=700 align='center'>\n",
    "\n",
    "The DTM illustrates the relationship between documents in rows and terms in columns. Each element in the DTM can be interpreted as a **term frequency**.\n",
    "\n",
    "Now, you can compare the similarity of two documents by evaluating how many of the terms are present and how frequent they are in each document. But a more mathematical approach may be takign the **dot product** (i.e. the sum of the product of corresponding elements) of the two vectors to get a numerical representation of that similarity:\n",
    "\n",
    "<img src=\"assets/images/img_04.png\" width=700 align='center'>\n",
    "\n",
    "The greater the dot product, the more similar the vectors, and thus the documents, are! The dot product has a flaw however. It only captures the portions of overlap. It is not affected by other values that are not in common. So, pairs that are very different can end up with the same dot product value as pairs that are very similar. An alternative is a measure of **cosine similarity**!\n",
    "\n",
    "**Cosine similarity** is the value from dividing the product of two vectors by the product of their magnitudes or euclidean norms\n",
    "$$\\text{cos}\\left(\\theta\\right)=\\frac{a \\cdot b}{||a|| \\cdot ||b||} = \\frac{1}{\\sqrt{3}\\cdot\\sqrt{3}}=\\frac{1}{3}$$\n",
    "\n",
    "If you think of these vectors as some arrows in n-dimensional space, then this is equal to the **cosine of the angle of theta between them**.\n",
    "\n",
    "Identical vectors have cosine similarity equal to 1, orthogonal (or indepedent, nothing in common) vectors have cosine similarity equal to 0, and vectors that are exactly opposite they have cosine similarity equal to -1.\n",
    "\n",
    "$$\\text{cos}\\left(\\theta\\right)\\in \\{-1,1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a49f2e8-ac8c-4a34-84a0-a694e4a0a099",
   "metadata": {},
   "source": [
    "## <a id='2'>2: Term Frequency Inverse Document Frequency (TF-IDF)</a>\n",
    "\n",
    "One limitation of the BoW approach is that it treats every word as being equally important. However, intuitively, we know that some words occur frequently within a corpus. We can compensate for this by *counting the number of times a word occurs in the corpus*. This is called the **document frequency**.\n",
    "\n",
    "<img src=\"assets/images/img_05.png\" width=700 align='center'>\n",
    "\n",
    "Now, we can use this to weight the terms in the DTM:\n",
    "\n",
    "<img src=\"assets/images/img_06.png\" width=700 align='center'>\n",
    "\n",
    "This gives us a metric that is proportional to the frequency of occurence of a term in a document, but *inversely proportional to the number of documents it appears in*. It highlights words that are more unique to a document and is thus better for characterizing the document.\n",
    "\n",
    "The **TF-IDF** is simply the product of two weights very similar to what we've seen so far. The most commonly used form of TF-IDF defines *term frequency* as the <u>raw count of a term $t$ in a document $d$ divided by the number of terms in $d$</u>. And, inverse document frequency as the <u>logarithm of the total number documents in the collection $d$ divided by the number of documents where $t$ is present</u>. There are alternatives that seek to normalize, or smooth, the resulting values, or prevent edge cases such as dividing by zero errors.\n",
    "\n",
    "<img src=\"assets/images/img_07.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e897599-ab9c-4c95-b770-fd3032913321",
   "metadata": {},
   "source": [
    "## <a id='3'>3: One-Hot Encoding (OHE)</a>\n",
    "\n",
    "If we treat our words like a class, assign a vector that has one in a single pre-determined position for that word and zero everywhere else:\n",
    "\n",
    "<img src=\"assets/images/img_08.png\" width=700 align='center'>\n",
    "\n",
    "This is very similar to the BoW idea, only that the keep a single word in each bag and build a vector for it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894bd14-c8e0-431d-be2b-c2e88ed679d2",
   "metadata": {},
   "source": [
    "## <a id='4'>4: Word Embeddings</a>\n",
    "\n",
    "OHE works in many situations, but breaks down when we have a large vocabulary to deal with because the size of our words representation grows with then number of words in a corpus. Another approach deals with addressing the size of our word representation by limiting it to a fixed size vector.\n",
    "\n",
    "In other words, we want to find an embedding for each word in a vector space and we want it to exhibit some desired properties. For example, if two words are similar in meaning, they should be closer together than two words that are dissimilar.\n",
    "\n",
    "<img src=\"assets/images/img_09.png\" width=700 align='center'>\n",
    "\n",
    "If two pairs of words have a similar difference in their meaning, they should be approximately equally separated in the embedding space.\n",
    "\n",
    "<img src=\"assets/images/img_10.png\" width=700 align='center'>\n",
    "\n",
    "We could use such a representation for a variety of purposes. Like, finding synonyms, analogies, identifying concepts around which words are clustered, classifying words as positive or negative or neutral, etc. By combining word vectors, we can come up with another way of representing documents as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33900760-000b-43ac-a27f-54e4b0df2931",
   "metadata": {},
   "source": [
    "## <a id='5'>5: Word2Vec</a>\n",
    "\n",
    "Word2Vec is one of the most popular examples of word embeddings used in practice. As the name indicates, it transforms words to vectors.\n",
    "\n",
    "The core idea of Word2Vec: a model that is able to **predict a given word, given neighboring words**, or vice versa, **predict neighboring words for a given word**, is likely to capture the contextual meanings of words very well.\n",
    "\n",
    "Two flavors of Word2Vec models:\n",
    "1. Continuous Bag-of-Word (CBoW)\n",
    "2. Continous Skip-gram\n",
    "\n",
    "<img src=\"assets/images/img_11.png\" width=700 align='center'>\n",
    "\n",
    "### Skip-gram Model:\n",
    "\n",
    "Pick a word from a sentence, convert it to an OHE vector, feed it into a probabilistic model (likely a neural network) that is designed to predict a few surrounding words, i.e. its context. You do this a number of times with a loss function to minimize until it predicts context words as best as it can.\n",
    "\n",
    "If you take an intermediate representation like a hidden layer in the neural network, the outputs of that layer for a given word become the corresponding word vector.\n",
    "\n",
    "<img src=\"assets/images/img_12.png\" width=700 align='center'>\n",
    "\n",
    "The CBoW variation also uses a similar strategy.\n",
    "\n",
    "Word2Vec properties:\n",
    "* Yields a robust meaning of the words because the meaning of each word is distributed throughout the vector (i.e. distributed representation)\n",
    "* Size of vector is up to you, you can think of it as a tradeoff of performance and complexity (i.e. the size is independent of the vocabulary), it remains constant no matter how many terms on which you train\n",
    "* Once pre-trained a large set of word vectors, you can use them efficiently without having to transform again and again (i.e. train once, store in lookup-table)\n",
    "* They are ready to be used in deep learning architectures, can be used as the input vector for recurrent neural nets (RNN) (i.e. deep learning ready)\n",
    "> * Possible to use RNNs to learn even better word embeddings\n",
    "\n",
    "There are other optimizations that are possible to further reduce the model and training complexity such as representing the output of the words using Hierarchical Softmax, computing loss using Sparse Cross Entropy, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b3351-9f75-48a4-9c9e-35897a238ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_nlp",
   "language": "python",
   "name": "venv_ud_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
