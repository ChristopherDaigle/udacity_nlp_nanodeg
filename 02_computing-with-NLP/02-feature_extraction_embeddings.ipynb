{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d20f04-bc06-4164-a030-22151e4ea568",
   "metadata": {},
   "source": [
    "# Feature Extraction and Embeddings\n",
    "\n",
    "[1. Bag of words](#1)<br>\n",
    "[2. Term Frequency Inverse Document Frequency (TF-IDF)](#2)<br>\n",
    "[3. One-hot encoding (OHE)](#3)<br>\n",
    "[4. Word embedding](#4)<br>\n",
    "[5. Word2Vec](#5)<br>\n",
    "[6. GloVe](#6)<br>\n",
    "[7. Embeddings for Deep Learning](#7)<br>\n",
    "[8. t-SNE](#8)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d99ff6-6895-4206-ab01-011879818868",
   "metadata": {},
   "source": [
    "Once we have our text cleaned and transformed, we need to transform it into features that can be used for modeling. In this section, we will cover methods for doing just that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67cdeb-2113-4666-a51b-6fa78872b202",
   "metadata": {},
   "source": [
    "# <a id='1'>1: Bag-of-Words (Bow)</a>\n",
    "\n",
    "The BoW model treats each document as an un-ordered collection or bag-of-words.\n",
    "\n",
    "A **document** is the *unit of text* you want to analyze. As an example, if you wanted to compare essays submitted by students for plagiarism, each essay would be a **document**.\n",
    "\n",
    "To obtain a BoW from a piece of raw text, you need to simply apply appropriate text processing (cleaning, normalizing, splitting into words, stemming, etc.) and then treat the resulting **tokens** as an *un-ordered* collection or set:\n",
    "\n",
    "$$\\text{Little House on the Prairie} \\rightarrow \\text{{\"littl\", \"hous\", \"prairi\"}}$$\n",
    "$$\\text{Mary had a Little Lamb} \\rightarrow \\text{{\"mari\", \"littl\", \"lamb\"}}$$\n",
    "$$\\text{The Silence of the Lambs} \\rightarrow \\text{{\"silenc\", \"lamb\"}}$$\n",
    "$$\\text{Twinkle Twinkle Little Star} \\rightarrow \\text{{\"twinkl\", \"littl\", \"star\"}}$$\n",
    "\n",
    "But keeping these as separate sets is very inefficient. They are of different sizes, may contain different words, and are hard to compare. What if a word occurs multiple times in a document? A more useful approach is turning each document into a vector of numbers representing how many times each word occurs in a document.\n",
    "\n",
    "A *set of documents* is known as a **corpus**. A corpus gives the context for the vectors to be calculated.\n",
    "\n",
    "**First:** collect all the unique words present in your corpus to form your vocabulary\n",
    "\n",
    "<img src=\"assets/images/img_01.png\" width=700 align='center'>\n",
    "\n",
    "**Second:** arrange these words in some order so that they form the vector element positions, or columns of a table, and assume each document is a row\n",
    "\n",
    "<img src=\"assets/images/img_02.png\" width=700 align='center'>\n",
    "\n",
    "**Third:** count the number of occurences of each word in each document and enter the value in the respective column, to create a **document-term matrix (DTM)**\n",
    "\n",
    "<img src=\"assets/images/img_03.png\" width=700 align='center'>\n",
    "\n",
    "The DTM illustrates the relationship between documents in rows and terms in columns. Each element in the DTM can be interpreted as a **term frequency**.\n",
    "\n",
    "Now, you can compare the similarity of two documents by evaluating how many of the terms are present and how frequent they are in each document. But a more mathematical approach may be takign the **dot product** (i.e. the sum of the product of corresponding elements) of the two vectors to get a numerical representation of that similarity:\n",
    "\n",
    "<img src=\"assets/images/img_04.png\" width=700 align='center'>\n",
    "\n",
    "The greater the dot product, the more similar the vectors, and thus the documents, are! The dot product has a flaw however. It only captures the portions of overlap. It is not affected by other values that are not in common. So, pairs that are very different can end up with the same dot product value as pairs that are very similar. An alternative is a measure of **cosine similarity**!\n",
    "\n",
    "**Cosine similarity** is the value from dividing the product of two vectors by the product of their magnitudes or euclidean norms\n",
    "$$\\text{cos}\\left(\\theta\\right)=\\frac{a \\cdot b}{||a|| \\cdot ||b||} = \\frac{1}{\\sqrt{3}\\cdot\\sqrt{3}}=\\frac{1}{3}$$\n",
    "\n",
    "If you think of these vectors as some arrows in n-dimensional space, then this is equal to the **cosine of the angle of theta between them**.\n",
    "\n",
    "Identical vectors have cosine similarity equal to 1, orthogonal (or indepedent, nothing in common) vectors have cosine similarity equal to 0, and vectors that are exactly opposite they have cosine similarity equal to -1.\n",
    "\n",
    "$$\\text{cos}\\left(\\theta\\right)\\in \\{-1,1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a49f2e8-ac8c-4a34-84a0-a694e4a0a099",
   "metadata": {},
   "source": [
    "# <a id='2'>2: Term Frequency Inverse Document Frequency (TF-IDF)</a>\n",
    "\n",
    "One limitation of the BoW approach is that it treats every word as being equally important. However, intuitively, we know that some words occur frequently within a corpus. We can compensate for this by *counting the number of times a word occurs in the corpus*. This is called the **document frequency**.\n",
    "\n",
    "<img src=\"assets/images/img_05.png\" width=700 align='center'>\n",
    "\n",
    "Now, we can use this to weight the terms in the DTM:\n",
    "\n",
    "<img src=\"assets/images/img_06.png\" width=700 align='center'>\n",
    "\n",
    "This gives us a metric that is proportional to the frequency of occurence of a term in a document, but *inversely proportional to the number of documents it appears in*. It highlights words that are more unique to a document and is thus better for characterizing the document.\n",
    "\n",
    "The **TF-IDF** is simply the product of two weights very similar to what we've seen so far. The most commonly used form of TF-IDF defines *term frequency* as the <u>raw count of a term $t$ in a document $d$ divided by the number of terms in $d$</u>. And, inverse document frequency as the <u>logarithm of the total number documents in the collection $d$ divided by the number of documents where $t$ is present</u>. There are alternatives that seek to normalize, or smooth, the resulting values, or prevent edge cases such as dividing by zero errors.\n",
    "\n",
    "<img src=\"assets/images/img_07.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e897599-ab9c-4c95-b770-fd3032913321",
   "metadata": {},
   "source": [
    "# <a id='3'>3: One-Hot Encoding (OHE)</a>\n",
    "\n",
    "If we treat our words like a class, assign a vector that has one in a single pre-determined position for that word and zero everywhere else:\n",
    "\n",
    "<img src=\"assets/images/img_08.png\" width=700 align='center'>\n",
    "\n",
    "This is very similar to the BoW idea, only that the keep a single word in each bag and build a vector for it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894bd14-c8e0-431d-be2b-c2e88ed679d2",
   "metadata": {},
   "source": [
    "# <a id='4'>4: Word Embeddings</a>\n",
    "\n",
    "OHE works in many situations, but breaks down when we have a large vocabulary to deal with because the size of our words representation grows with then number of words in a corpus. Another approach deals with addressing the size of our word representation by limiting it to a fixed size vector.\n",
    "\n",
    "In other words, we want to find an embedding for each word in a vector space and we want it to exhibit some desired properties. For example, if two words are similar in meaning, they should be closer together than two words that are dissimilar.\n",
    "\n",
    "<img src=\"assets/images/img_09.png\" width=700 align='center'>\n",
    "\n",
    "If two pairs of words have a similar difference in their meaning, they should be approximately equally separated in the embedding space.\n",
    "\n",
    "<img src=\"assets/images/img_10.png\" width=700 align='center'>\n",
    "\n",
    "We could use such a representation for a variety of purposes. Like, finding synonyms, analogies, identifying concepts around which words are clustered, classifying words as positive or negative or neutral, etc. By combining word vectors, we can come up with another way of representing documents as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33900760-000b-43ac-a27f-54e4b0df2931",
   "metadata": {},
   "source": [
    "# <a id='5'>5: Word2Vec</a>\n",
    "\n",
    "Word2Vec is one of the most popular examples of word embeddings used in practice. As the name indicates, it transforms words to vectors.\n",
    "\n",
    "The core idea of Word2Vec: a model that is able to **predict a given word, given neighboring words**, or vice versa, **predict neighboring words for a given word**, is likely to capture the contextual meanings of words very well.\n",
    "\n",
    "Two flavors of Word2Vec models:\n",
    "1. Continuous Bag-of-Word (CBoW)\n",
    "2. Continous Skip-gram\n",
    "\n",
    "<img src=\"assets/images/img_11.png\" width=700 align='center'>\n",
    "\n",
    "### Skip-gram Model:\n",
    "\n",
    "Pick a word from a sentence, convert it to an OHE vector, feed it into a probabilistic model (likely a neural network) that is designed to predict a few surrounding words, i.e. its context. You do this a number of times with a loss function to minimize until it predicts context words as best as it can.\n",
    "\n",
    "If you take an intermediate representation like a hidden layer in the neural network, the outputs of that layer for a given word become the corresponding word vector.\n",
    "\n",
    "<img src=\"assets/images/img_12.png\" width=700 align='center'>\n",
    "\n",
    "The CBoW variation also uses a similar strategy.\n",
    "\n",
    "Word2Vec properties:\n",
    "* Yields a robust meaning of the words because the meaning of each word is distributed throughout the vector (i.e. distributed representation)\n",
    "* Size of vector is up to you, you can think of it as a tradeoff of performance and complexity (i.e. the size is independent of the vocabulary), it remains constant no matter how many terms on which you train\n",
    "* Once pre-trained a large set of word vectors, you can use them efficiently without having to transform again and again (i.e. train once, store in lookup-table)\n",
    "* They are ready to be used in deep learning architectures, can be used as the input vector for recurrent neural nets (RNN) (i.e. deep learning ready)\n",
    "> * Possible to use RNNs to learn even better word embeddings\n",
    "\n",
    "There are other optimizations that are possible to further reduce the model and training complexity such as representing the output of the words using Hierarchical Softmax, computing loss using Sparse Cross Entropy, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f92264-9d45-4554-8282-6707c55f82de",
   "metadata": {},
   "source": [
    "# <a id='6'>6: GloVe</a>\n",
    "\n",
    "**GloVe**, or *Global Vectors* for word representation tries to directly optimize the vector represenation of each word just using co-occurence statistics; unlike Word2Vec that sets up an ancillary prediction task.\n",
    "\n",
    "**First**, the probability that word $j$ appears in the context of word $i$ is computed, $p(j|i)$ in a given corpus for all word pairs $ij$ in a given corpus\n",
    "> * $j$ appears in context of $i$ means that word $j$ is present in the vicinity of word $i$, either right next to it or a few words away\n",
    "\n",
    "<img src=\"assets/images/img_13.png\" width=700 align='center'>\n",
    "\n",
    "**Second**, count all such occurences of $i$ and $j$ in our corpus, then normalize the count to get a probability of any such occurence\n",
    "\n",
    "**Third**, two random vectors are initialized for each word. One vector for the word when it is acting as a context. One for when it is the target.\n",
    "\n",
    "<img src=\"assets/images/img_14.png\" width=700 align='center'>\n",
    "\n",
    "**Fourth**, for any pair of words $i$ $j$, we want the dot-product of their word vectors, $w_{i} \\cdot w_{j}$ , to be equal to their co-occurence probability.\n",
    "> * Using this as our goal and a suitable loss function, we can iteratively optimize these word vectors\n",
    "\n",
    "The **result** should be a set of vectors that captures the similarities and differences between individual words\n",
    "\n",
    "<img src=\"assets/images/img_15.png\" width=700 align='center'>\n",
    "\n",
    "From a different perspective, we are *factorizing the co-occurence probability matrix, $p(i|j)$, into two smaller matrices*. This is the basic idea behind GloVe.\n",
    "\n",
    "<img src=\"assets/images/img_16.png\" width=700 align='center'>\n",
    "\n",
    "**Why do co-occurence probabilities matter?**\n",
    "\n",
    "As an example, consider:\n",
    "> * Context words: ice & steam\n",
    "> * Target words: solid & water\n",
    "\n",
    "|Context/Target|solid|water|\n",
    "|:-------------|:---:|:---:|\n",
    "|**ice**| p(solid \\| ice) | p(water \\| ice) |\n",
    "|**steam**| p(solid \\| steam) | p(water \\| steam) |\n",
    "\n",
    "$$p(solidice)>p(solid|steam)$$\n",
    "$$p(water|ice)\\approx p(water|steam)$$\n",
    "$$\\rightarrow$$\n",
    "$$\\frac{p(solid|ice)}{p(solid|steam)} > 1 \\land \\frac{p(water|ice)}{p(water|steam)} \\approx 1$$\n",
    "\n",
    "We would expect that \"solid\" and \"ice\" appear much more frequently together than \"solid\" and \"steam\" and that is in fact what co-occurence probabilities capture.\n",
    "\n",
    "One refinement over using raw probabilities is to optimize for the ratio of probabilities.\n",
    "\n",
    "* There are a lot of subtleties here, especially the the co-occurence matrix is huge and sparse (many co-occurence probabilities are very low)\n",
    "* It makes sense to work with the log of these values\n",
    "\n",
    "[Here is the original paper that introduced GloVe](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d373d-522e-410e-bce5-ed386d75a4b0",
   "metadata": {},
   "source": [
    "# <a id='7'>7: Embeddings for Deep Learning</a>\n",
    "\n",
    "Word embeddings have become the de facto choice for representing words, especially for use in deep neural networks\n",
    "\n",
    "Why do these techniques work so well? The ability to perfrom arithmetic with words seems almost magical:\n",
    "\n",
    "$$\\text{woman} - \\text{man} + \\text{king} = \\text{queen}$$\n",
    "\n",
    "<img src=\"assets/images/img_17.png\" width=700 align='center'>\n",
    "\n",
    "The answer to *why* might lie in the distributional hypothesis, which states that words that occur in same contexts tend to have similar meanings\n",
    "\n",
    "<img src=\"assets/images/img_18.png\" width=700 align='center'>\n",
    "\n",
    "The words **cup**, **black**, and **morning** give you a hint that maybe the **blank** word is *coffee* or *tea*. It could be either of the two. That's the point. *In these contexts, tea and coffee are very similar*\n",
    "\n",
    "When a large collection of documents is used to learn a term's embedding, words with common context tend to get pulled closer and closer together, and vice versa, words that are dissimilar tend to get pushed further apart.\n",
    "\n",
    "<img src=\"assets/images/img_19.png\" width=700 align='center'>\n",
    "\n",
    "In some context though, **coffee** and **tea** may be further apart:\n",
    "* ____ grounds are great for composting\n",
    "* I prefer loose leaf ____\n",
    "\n",
    "**How do we capture these similarities AND differences in the same embedding?** *By adding another dimension!\n",
    "\n",
    "**Tea** and **coffee** may be close along one dimension, but separate across another dimension. Maybe another dimension to compare **tea** and **coffee** could be *variability among beverages*.\n",
    "\n",
    "<img src=\"assets/images/img_20.png\" width=700 align='center'>\n",
    "\n",
    "In human language, there are many dimensions along which terms can vary, and the more dimensions you can capture in your word vector, the more expressive that representation will be. But how many do you really need?\n",
    "\n",
    "If you take in a OHE word vector, and learn an embedding, the embedding may be magnitudes smaller than the input. For example an OHE vector representing $10^{5}$ (as large as the size of the vocabulary) dimensions to an embedding of a few hundred dimensions, which is significantly smaller than the input (especially if it's an OHE).\n",
    "\n",
    "If you learn an embedding from the training process, you can capture the dimensions that are most relevant for your task. This method often adds complexity and is most useful for a very narrow application, like one that deals with medical terminology.\n",
    "\n",
    "<img src=\"assets/images/img_21.png\" width=350 align='center'>\n",
    "\n",
    "\n",
    "It is simpler to use a pre-trained embedding as a look-up, like Word2Vec or GloVe. Then you only need to train the layer specific to your task.\n",
    "\n",
    "<img src=\"assets/images/img_22.png\" width=350 align='center'>\n",
    "\n",
    "This is similar to Computer Vision tasks where the early stages are usually able to leverage a pre-trained network, like AlexNET or VGG-16, and then learn only the later layers\n",
    "\n",
    "<img src=\"assets/images/img_23.png\" width=350 align='center'>\n",
    "\n",
    "Both of these are examples of transfer learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a09ee-1a8c-456e-b9b9-0f7b0fb85105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_nlp",
   "language": "python",
   "name": "venv_ud_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
