{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d20f04-bc06-4164-a030-22151e4ea568",
   "metadata": {},
   "source": [
    "# Feature Extraction and Embeddings\n",
    "\n",
    "[1. Bag of words](#1)<br>\n",
    "[2. Term Frequency Inverse Document Frequency (TF-IDF)](#2)<br>\n",
    "[3. One-hot encoding (OHE)](#3)<br>\n",
    "[4. Word embedding](#4)<br>\n",
    "[5. Word2Vec](#5)<br>\n",
    "[6. GloVe](#6)<br>\n",
    "[7. Embeddings for deep learning](#7)<br>\n",
    "[8. t-SNE](#8)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d99ff6-6895-4206-ab01-011879818868",
   "metadata": {},
   "source": [
    "Once we have our text cleaned and transformed, we need to transform it into features that can be used for modeling. In this section, we will cover methods for doing just that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67cdeb-2113-4666-a51b-6fa78872b202",
   "metadata": {},
   "source": [
    "## <a id='1'>1: Bag-of-Words (Bow)</a>\n",
    "\n",
    "The BoW model treats each document as an un-ordered collection or bag-of-words.\n",
    "\n",
    "A **document** is the *unit of text* you want to analyze. As an example, if you wanted to compare essays submitted by students for plagiarism, each essay would be a **document**.\n",
    "\n",
    "To obtain a BoW from a piece of raw text, you need to simply apply appropriate text processing (cleaning, normalizing, splitting into words, stemming, etc.) and then treat the resulting **tokens** as an *un-ordered* collection or set:\n",
    "\n",
    "$$\\text{Little House on the Prairie} \\rightarrow \\text{{\"littl\", \"hous\", \"prairi\"}}$$\n",
    "$$\\text{Mary had a Little Lamb} \\rightarrow \\text{{\"mari\", \"littl\", \"lamb\"}}$$\n",
    "$$\\text{The Silence of the Lambs} \\rightarrow \\text{{\"silenc\", \"lamb\"}}$$\n",
    "$$\\text{Twinkle Twinkle Little Star} \\rightarrow \\text{{\"twinkl\", \"littl\", \"star\"}}$$\n",
    "\n",
    "But keeping these as separate sets is very inefficient. They are of different sizes, may contain different words, and are hard to compare. What if a word occurs multiple times in a document? A more useful approach is turning each document into a vector of numbers representing how many times each word occurs in a document.\n",
    "\n",
    "A *set of documents* is known as a **corpus**. A corpus gives the context for the vectors to be calculated.\n",
    "\n",
    "**First:** collect all the unique words present in your corpus to form your vocabulary\n",
    "\n",
    "<img src=\"assets/images/img_01.png\" width=700 align='center'>\n",
    "\n",
    "**Second:** arrange these words in some order so that they form the vector element positions, or columns of a table, and assume each document is a row\n",
    "\n",
    "<img src=\"assets/images/img_02.png\" width=700 align='center'>\n",
    "\n",
    "**Third:** count the number of occurences of each word in each document and enter the value in the respective column, to create a **document-term matrix (DTM)**\n",
    "\n",
    "<img src=\"assets/images/img_03.png\" width=700 align='center'>\n",
    "\n",
    "The DTM illustrates the relationship between documents in rows and terms in columns. Each element in the DTM can be interpreted as a **term frequency**.\n",
    "\n",
    "Now, you can compare the similarity of two documents by evaluating how many of the terms are present and how frequent they are in each document. But a more mathematical approach may be takign the **dot product** (i.e. the sum of the product of corresponding elements) of the two vectors to get a numerical representation of that similarity:\n",
    "\n",
    "<img src=\"assets/images/img_04.png\" width=700 align='center'>\n",
    "\n",
    "The greater the dot product, the more similar the vectors, and thus the documents, are! The dot product has a flaw however. It only captures the portions of overlap. It is not affected by other values that are not in common. So, pairs that are very different can end up with the same dot product value as pairs that are very similar. An alternative is a measure of **cosine similarity**!\n",
    "\n",
    "**Cosine similarity** is the value from dividing the product of two vectors by the product of their magnitudes or euclidean norms\n",
    "$$\\text{cos}\\left(\\theta\\right)=\\frac{a \\cdot b}{||a|| \\cdot ||b||} = \\frac{1}{\\sqrt{3}\\cdot\\sqrt{3}}=\\frac{1}{3}$$\n",
    "\n",
    "If you think of these vectors as some arrows in n-dimensional space, then this is equal to the **cosine of the angle of theta between them**.\n",
    "\n",
    "Identical vectors have cosine similarity equal to 1, orthogonal (or indepedent, nothing in common) vectors have cosine similarity equal to 0, and vectors that are exactly opposite they have cosine similarity equal to -1.\n",
    "\n",
    "$$\\text{cos}\\left(\\theta\\right)\\in \\{-1,1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a49f2e8-ac8c-4a34-84a0-a694e4a0a099",
   "metadata": {},
   "source": [
    "## <a id='2'>2: Term Frequency Inverse Document Frequency (TF-IDF)</a>\n",
    "\n",
    "One limitation of the BoW approach is that it treats every word as being equally important. However, intuitively, we know that some words occur frequently within a corpus. We can compensate for this by *counting the number of times a word occurs in the corpus*. This is called the **document frequency**.\n",
    "\n",
    "<img src=\"assets/images/img_05.png\" width=700 align='center'>\n",
    "\n",
    "Now, we can use this to weight the terms in the DTM:\n",
    "\n",
    "<img src=\"assets/images/img_06.png\" width=700 align='center'>\n",
    "\n",
    "This gives us a metric that is proportional to the frequency of occurence of a term in a document, but *inversely proportional to the number of documents it appears in*. It highlights words that are more unique to a document and is thus better for characterizing the document.\n",
    "\n",
    "The **TF-IDF** is simply the product of two weights very similar to what we've seen so far. The most commonly used form of TF-IDF defines *term frequency* as the <u>raw count of a term $t$ in a document $d$ divided by the number of terms in $d$</u>. And, inverse document frequency as the <u>logarithm of the total number documents in the collection $d$ divided by the number of documents where $t$ is present</u>. There are alternatives that seek to normalize, or smooth, the resulting values, or prevent edge cases such as dividing by zero errors.\n",
    "\n",
    "<img src=\"assets/images/img_07.png\" width=700 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e897599-ab9c-4c95-b770-fd3032913321",
   "metadata": {},
   "source": [
    "## <a id='3'>3: One-Hot Encoding (OHE)</a>\n",
    "\n",
    "If we treat our words like a class, assign a vector that has one in a single pre-determined position for that word and zero everywhere else:\n",
    "\n",
    "<img src=\"assets/images/img_08.png\" width=700 align='center'>\n",
    "\n",
    "This is very similar to the BoW idea, only that the keep a single word in each bag and build a vector for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a9aab-aa57-435a-966f-ff4121ec2c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_nlp",
   "language": "python",
   "name": "venv_ud_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
